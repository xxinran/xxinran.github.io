<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>cpu-microarch</title>
    <url>/posts/f1d93d2c.html</url>
    <content><![CDATA[<p><img src="https://en.wikichip.org/w/images/thumb/e/ee/skylake_server_block_diagram.svg/950px-skylake_server_block_diagram.svg.png" alt="skylake server block diagram.svg"></p>
<p>front end：取值，解码</p>
<p>execution engine：计算</p>
<p>memory subsystem： 读写内存</p>
<p>L1， L2 per core</p>
]]></content>
  </entry>
  <entry>
    <title>cpu基础</title>
    <url>/posts/fe9b70.html</url>
    <content><![CDATA[<p>cpu运行阶段：取值，解码，执行，写回。</p>
<p>cpu从内存或者cache里提取指令，放入指令寄存器，并对指令译码，执行。</p>
<p>主要参数：</p>
<p>主频：时钟频率</p>
<p>倍频</p>
<p>外频：cpu的基准频率，cpu和其他部件通讯的频率，决定着整块主板的运行速度。因为cpu的频率太高，其他部件跟不上。</p>
<p>超频：</p>
]]></content>
  </entry>
  <entry>
    <title>dapr-intro</title>
    <url>/posts/c02c2ddc.html</url>
    <content><![CDATA[<h2 id="dapr简介"><a href="#dapr简介" class="headerlink" title="dapr简介"></a>dapr简介</h2><h2 id="dapr安装"><a href="#dapr安装" class="headerlink" title="dapr安装"></a>dapr安装</h2><p>dapr支持多种运行环境：</p>
<ol>
<li>在本机上，相当于一个process</li>
<li>在kubernetes中，和业务容器在一个pod里，作为sidecar</li>
</ol>
<p>我们先尝试在第一种环境里安装dapr。</p>
<h4 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h4><p>Docker installed</p>
<h4 id="安装dapr-CLI"><a href="#安装dapr-CLI" class="headerlink" title="安装dapr CLI"></a>安装dapr CLI</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wget -q https://raw.githubusercontent.com/dapr/cli/master/install/install.sh -O - | /bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> verify</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> dapr --version</span></span><br></pre></td></tr></table></figure>
<h4 id="初始化dapr"><a href="#初始化dapr" class="headerlink" title="初始化dapr"></a>初始化dapr</h4><p>因为dapr在host上直接运行，相当于需要在host上运行一个process。所以dapr的初始化包括下载dapr二进制包并且安装运行。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dapr init</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在k8s上运行 dapr init --kubernetes</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> verify</span> </span><br><span class="line"><span class="meta">$</span><span class="bash"> docker ps</span></span><br><span class="line">CONTAINER ID        IMAGE                                 COMMAND                  CREATED             STATUS                 PORTS                                                                                                                                  NAMES</span><br><span class="line">caa1ac7a14c4        openzipkin/zipkin                     &quot;start-zipkin&quot;           2 hours ago         Up 2 hours (healthy)   9410/tcp, 0.0.0.0:9411-&gt;9411/tcp                                                                                                       dapr_zipkin</span><br><span class="line">1fa7739631a3        redis                                 &quot;docker-entrypoint.s…&quot;   2 hours ago         Up 2 hours             0.0.0.0:6379-&gt;6379/tcp                                                                                                                 dapr_redis</span><br><span class="line">f48ee8e3d872        daprio/dapr                           &quot;./placement&quot;            2 hours ago         Up 2 hours             0.0.0.0:50005-&gt;50005/tcp                                                                                                               dapr_placement</span><br></pre></td></tr></table></figure>
<p>可以看到有三个容器在运行，Redis容器是做state management和messaging的，Zipkin容器是为了收集traces的。</p>
<p>在<code>docker init</code>的时候，同时还创建了一些文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls <span class="variable">$HOME</span>/.dapr</span></span><br><span class="line">bin/  components/  config.yaml</span><br></pre></td></tr></table></figure>


<h3 id="使用dapr-API，run-sample"><a href="#使用dapr-API，run-sample" class="headerlink" title="使用dapr API，run sample"></a>使用dapr API，run sample</h3><p>业务代码通过调用dapr API来完成一些业务之外的分布式服务的功能，所以我们可以通过直接调用dapr API来看看dapr API是怎么被使用的。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dapr run --<span class="built_in">help</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run a Python application that listens to port 3000:</span></span><br><span class="line">  dapr run --app-id myapp --app-port 3000 -- python myapp.py</span><br><span class="line"><span class="meta">#</span><span class="bash"> Run sidecar only:</span></span><br><span class="line">  dapr run --app-id myapp</span><br><span class="line"></span><br><span class="line">Flags:</span><br><span class="line"><span class="meta"> #</span><span class="bash"> 需要研究这个app-id有啥用？？？</span></span><br><span class="line">  -a, --app-id string                   The id for your application, used for service discovery</span><br><span class="line">      --app-max-concurrency int         The concurrency level of the application, otherwise is unlimited (default -1)</span><br><span class="line"><span class="meta"> #</span><span class="bash"> 业务程序监听的端口</span></span><br><span class="line">  -p, --app-port int                    The port your application is listening on (default -1)</span><br><span class="line"><span class="meta"> #</span><span class="bash"> dapr和app之间的protocol</span></span><br><span class="line">  -P, --app-protocol string             The protocol (gRPC or HTTP) Dapr uses to talk to the application (default &quot;http&quot;)</span><br><span class="line">      --app-ssl                         Enable https when Dapr invokes the application</span><br><span class="line">  -d, --components-path string          The path for components directory (default &quot;/home/ubuntu/.dapr/components&quot;)</span><br><span class="line"> -c, --config string                   Dapr configuration file (default &quot;/home/ubuntu/.dapr/config.yaml&quot;)</span><br><span class="line"><span class="meta"> #</span><span class="bash"> dapr程序监听的端口s</span></span><br><span class="line">  -G, --dapr-grpc-port int              The gRPC port for Dapr to listen on (default -1)</span><br><span class="line">  -H, --dapr-http-port int              The HTTP port for Dapr to listen on (default -1)</span><br><span class="line">      --enable-profiling                Enable pprof profiling via an HTTP endpoint</span><br><span class="line">  -h, --help                            Print this help message</span><br><span class="line">      --log-level string                The log verbosity. Valid values are: debug, info, warn, error, fatal, or panic (default &quot;info&quot;)</span><br><span class="line">  -M, --metrics-port int                The port of metrics on dapr (default -1)</span><br><span class="line">      --placement-host-address string   The host on which the placement service resides (default &quot;localhost&quot;)</span><br><span class="line">      --profile-port int                The port for the profile server to listen on (default -1)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> launch a Dapr sidecar that will listen on port 3500 <span class="keyword">for</span> a blank application named myapp</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 因为没有--config和-d，所以用的就是default的`<span class="variable">$HOME</span>/.dapr`下的配置文件。</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> dapr run --app-id myapp --dapr-http-port 3500</span></span><br></pre></td></tr></table></figure>
<p><strong>调用dapr API去写state，然后读：</strong></p>
<p>这里是写state，所以url是/v1.0/state/statestore，这个statestore是<code>$HOME/.dapr/components/statestore.yaml</code>里面定义的。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -X POST -H <span class="string">&quot;Content-Type: application/json&quot;</span> -d <span class="string">&#x27;[&#123; &quot;key&quot;: &quot;name&quot;, &quot;value&quot;: &quot;Bruce Wayne&quot;&#125;]&#x27;</span> http://localhost:3500/v1.0/state/statestore</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> curl http://localhost:3500/v1.0/state/statestore/name</span></span><br><span class="line">&quot;Bruce Wayne&quot;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入redis容器查看在redis中的存储形式</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> -it dapr_redis redis-cli</span></span><br><span class="line">127.0.0.1:6379&gt; keys *</span><br><span class="line">1) &quot;myapp||name&quot;   # key使用的是“app的名字||name”</span><br><span class="line">127.0.0.1:6379&gt; hgetall &quot;myapp||name&quot;</span><br><span class="line">1) &quot;data&quot;</span><br><span class="line">2) &quot;\&quot;Bruce Wayne\&quot;&quot;</span><br><span class="line">3) &quot;version&quot;</span><br><span class="line">4) &quot;2&quot;</span><br></pre></td></tr></table></figure>
<h3 id="自己定义component"><a href="#自己定义component" class="headerlink" title="自己定义component"></a>自己定义component</h3><p>上述我们用的都是dapr init时生成的简单的config例子。现在我们想自己定义新的component并使用dapr API去调用它（们）？</p>
<p>我们将：</p>
<ol>
<li>创建一个本地的json的secret store</li>
<li>把这个secret store写在component的定义里，向dapr注册。</li>
<li>通过dapr API获得这个secret store里的内容。</li>
</ol>
<h4 id="创建本地secret-store"><a href="#创建本地secret-store" class="headerlink" title="创建本地secret store"></a>创建本地secret store</h4><p>创建json文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat mysecrets.json</span></span><br><span class="line">&#123;</span><br><span class="line">     &quot;my-secret&quot; : &quot;I&#x27;m Batman&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">~/.dapr/my-components$ cat localSecretStore.yaml</span><br><span class="line">apiVersion: dapr.io/v1alpha1</span><br><span class="line">kind: Component</span><br><span class="line">metadata:</span><br><span class="line">  name: my-secret-store  # secret store的名字，在api调用时要用到</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  type: secretstores.local.file  # 指定类型时secretstore，而且是本地文件存储</span><br><span class="line">  version: v1</span><br><span class="line">  metadata:</span><br><span class="line">  - name: secretsFile</span><br><span class="line">    value: /home/ubuntu/.dapr/my-components/mysecrets.json</span><br><span class="line">  - name: nestedSeparator</span><br><span class="line">    value: &quot;:&quot;</span><br></pre></td></tr></table></figure>
<h4 id="运行一个dapr的process"><a href="#运行一个dapr的process" class="headerlink" title="运行一个dapr的process"></a>运行一个dapr的process</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dapr run --app-id myapp --dapr-http-port 3500 --components-path ./my-components</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> verify</span> </span><br><span class="line"><span class="meta">$</span><span class="bash"> curl localhost:3500/v1.0/secrets/my-secret-store/my-secret</span></span><br><span class="line">&#123;&quot;my-secret&quot;:&quot;I&#x27;m Batman&quot;&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>问题：怎么知道一个component时secret还是state还是别的？</p>
<p>答：通过yaml文件定义component是指定的type，这样在call API时就知道URL怎么写了。（是/v1.0/secrets还是v1.0/state等等）</p>
</blockquote>
<h2 id="在kubernetes集群上安装dapr"><a href="#在kubernetes集群上安装dapr" class="headerlink" title="在kubernetes集群上安装dapr"></a>在kubernetes集群上安装dapr</h2><p>minikube</p>
<p><img src="/posts/c02c2ddc/Architecture_Diagram.png" alt="Architecture Diagram"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dapr init -k</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> dapr status -k</span></span><br><span class="line">  NAME                   NAMESPACE    HEALTHY  STATUS   REPLICAS  VERSION  AGE  CREATED</span><br><span class="line">  dapr-placement-server  dapr-system  True     Running  1         1.0.1    2m   2021-03-09 15:02.06</span><br><span class="line">  dapr-dashboard         dapr-system  True     Running  1         0.6.0    2m   2021-03-09 15:02.05</span><br><span class="line">  dapr-sentry            dapr-system  True     Running  1         1.0.1    2m   2021-03-09 15:02.05</span><br><span class="line">  dapr-sidecar-injector  dapr-system  True     Running  1         1.0.1    2m   2021-03-09 15:02.05</span><br><span class="line">  dapr-operator          dapr-system  True     Running  1         1.0.1    2m   2021-03-09 15:02.05</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 5个dapr的组件</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> k get pods -ndapr-system</span></span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE</span><br><span class="line">dapr-dashboard-6bb75f9645-8f28z         1/1     Running   0          6m24s</span><br><span class="line">dapr-operator-5f84d9fd4-7smdf           1/1     Running   0          6m24s</span><br><span class="line">dapr-placement-server-0                 1/1     Running   0          6m23s</span><br><span class="line">dapr-sentry-5d67bfbfdc-fzsb6            1/1     Running   0          6m24s</span><br><span class="line">dapr-sidecar-injector-7b556f95d-kpxnc   1/1     Running   0          6m24s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意： placment 没有deployment，只有pod和service。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> k get deploy -ndapr-system</span></span><br><span class="line">NAME                    READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">dapr-dashboard          1/1     1            1           12d</span><br><span class="line">dapr-operator           1/1     1            1           12d</span><br><span class="line">dapr-sentry             1/1     1            1           12d</span><br><span class="line">dapr-sidecar-injector   1/1     1            1           12d</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="运行helloworld-sample"><a href="#运行helloworld-sample" class="headerlink" title="运行helloworld sample"></a>运行helloworld sample</h3><h4 id="安装redis作为state-store"><a href="#安装redis作为state-store" class="headerlink" title="安装redis作为state store"></a>安装redis作为state store</h4><h5 id="安装helm"><a href="#安装helm" class="headerlink" title="安装helm"></a>安装helm</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl https://baltocdn.com/helm/signing.asc | sudo apt-key add -</span><br><span class="line">sudo apt-get install apt-transport-https --yes</span><br><span class="line">echo &quot;deb https://baltocdn.com/helm/stable/debian/ all main&quot; | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install helm</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="安装redis"><a href="#安装redis" class="headerlink" title="安装redis"></a>安装redis</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">helm repo add bitnami https://charts.bitnami.com/bitnami</span><br><span class="line">helm repo update</span><br><span class="line">helm install redis bitnami/redis</span><br><span class="line"></span><br><span class="line">To get your password run:</span><br><span class="line"></span><br><span class="line">    export REDIS_PASSWORD=$(kubectl get secret --namespace default redis -o jsonpath=&quot;&#123;.data.redis-password&#125;&quot; | base64 --decode)</span><br><span class="line"></span><br><span class="line">To connect to your Redis(TM) server:</span><br><span class="line"></span><br><span class="line">1. Run a Redis(TM) pod that you can use as a client:</span><br><span class="line">   kubectl run --namespace default redis-client --rm --tty -i --restart=&#x27;Never&#x27; \</span><br><span class="line">    --env REDIS_PASSWORD=$REDIS_PASSWORD \</span><br><span class="line">   --image docker.io/bitnami/redis:6.0.12-debian-10-r3 -- bash</span><br><span class="line"></span><br><span class="line">2. Connect using the Redis(TM) CLI:</span><br><span class="line">   redis-cli -h redis-master -a $REDIS_PASSWORD</span><br><span class="line">   redis-cli -h redis-slave -a $REDIS_PASSWORD</span><br><span class="line"></span><br><span class="line">To connect to your database from outside the cluster execute the following commands:</span><br><span class="line"></span><br><span class="line">    kubectl port-forward --namespace default svc/redis-master 6379:6379 &amp;</span><br><span class="line">    redis-cli -h 127.0.0.1 -p 6379 -a $REDIS_PASSWORD</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> k get pods</span> </span><br><span class="line">default       redis-headless          ClusterIP   None             &lt;none&gt;        6379/TCP                 96s</span><br><span class="line">default       redis-master            ClusterIP   10.107.253.143   &lt;none&gt;        6379/TCP                 96s</span><br><span class="line">default       redis-slave             ClusterIP   10.102.228.172   &lt;none&gt;        6379/TCP                 96s</span><br><span class="line"><span class="meta">$</span><span class="bash"> k get secrets</span></span><br><span class="line">NAME                          TYPE                                  DATA   AGE</span><br><span class="line">default-token-hnmgm           kubernetes.io/service-account-token   3      155m</span><br><span class="line">redis                         Opaque                                1      3m4s</span><br><span class="line">sh.helm.release.v1.redis.v1   helm.sh/release.v1                    1      3m4s</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> k get svc -owide</span></span><br><span class="line">NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE     SELECTOR</span><br><span class="line">kubernetes       ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    5d13h   &lt;none&gt;</span><br><span class="line">redis-headless   ClusterIP   None             &lt;none&gt;        6379/TCP   5d10h   app=redis,release=redis</span><br><span class="line">redis-master     ClusterIP   10.107.253.143   &lt;none&gt;        6379/TCP   5d10h   app=redis,release=redis,role=master</span><br><span class="line">redis-slave      ClusterIP   10.102.228.172   &lt;none&gt;        6379/TCP   5d10h   app=redis,release=redis,role=slave</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash">  k get ep</span></span><br><span class="line">NAME             ENDPOINTS                                          AGE</span><br><span class="line">kubernetes       192.168.49.2:8443                                  5d13h</span><br><span class="line">redis-headless   172.17.0.10:6379,172.17.0.8:6379,172.17.0.9:6379   5d10h</span><br><span class="line">redis-master     172.17.0.9:6379                                    5d10h</span><br><span class="line">redis-slave      172.17.0.10:6379,172.17.0.8:6379                   5d10h</span><br></pre></td></tr></table></figure>




<p><img src="/posts/c02c2ddc/image-20210309233804291.png" alt="image-20210309233804291"></p>
<p>minikube user</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> minikube service nodeapp</span></span><br><span class="line">|-----------|---------|-------------|---------------------------|</span><br><span class="line">| NAMESPACE |  NAME   | TARGET PORT |            URL            |</span><br><span class="line">|-----------|---------|-------------|---------------------------|</span><br><span class="line">| default   | nodeapp |          80 | http://192.168.49.2:32108 |</span><br><span class="line">|-----------|---------|-------------|---------------------------|</span><br><span class="line">* Opening service default/nodeapp in default browser...</span><br><span class="line">  - http://192.168.49.2:32108</span><br><span class="line">  </span><br><span class="line"><span class="meta">#</span><span class="bash"> curl localhost:32108不通，因为用的是minikube</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> curl http://192.168.49.2:32108/ports</span></span><br><span class="line">&#123;&quot;DAPR_HTTP_PORT&quot;:&quot;3500&quot;,&quot;DAPR_GRPC_PORT&quot;:&quot;50001&quot;&#125;</span><br><span class="line"></span><br><span class="line">export NODE_APP=192.168.49.2:32108</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Next submit an order to the app</p>
<figure class="highlight swift"><table><tr><td class="code"><pre><span class="line">curl <span class="operator">--</span>request <span class="type">POST</span> <span class="operator">--</span>data <span class="string">&quot;&#123;<span class="subst">\&quot;</span>data<span class="subst">\&quot;</span>: &#123; <span class="subst">\&quot;</span>orderId<span class="subst">\&quot;</span>: <span class="subst">\&quot;</span>42<span class="subst">\&quot;</span> &#125; &#125;&quot;</span> <span class="operator">--</span>header <span class="string">&quot;Content-Type:application/json&quot;</span> http:<span class="comment">//$NODE_APP/neworder</span></span><br></pre></td></tr></table></figure>
<p>Expected output: Empty reply from server</p>
<p>Confirm the order was persisted by requesting it from the app</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">curl http:<span class="regexp">//</span><span class="variable">$NODE_APP</span>/order</span><br></pre></td></tr></table></figure>
<p>Expected output:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;orderId&quot;:&quot;42&quot;&#125;</span><br></pre></td></tr></table></figure>








<p>dapr cli</p>
<p>dapr host</p>
<p>dapr API</p>
<p>dapr runtime</p>
<p>dapr operator </p>
<p>dapr sidecar injector</p>
<p>dapr placement service</p>
<p>dapr sentry</p>
<p>building blocks</p>
]]></content>
      <categories>
        <category>cloud native</category>
      </categories>
  </entry>
  <entry>
    <title>如何构建docker image</title>
    <url>/posts/85a08636.html</url>
    <content><![CDATA[<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker image inspect ubuntu</span></span><br><span class="line">        &quot;RootFS&quot;: &#123;</span><br><span class="line">            &quot;Type&quot;: &quot;layers&quot;,</span><br><span class="line">            &quot;Layers&quot;: [</span><br><span class="line">                &quot;sha256:bacd3af13903e13a43fe87b6944acd1ff21024132aad6e74b4452d984fb1a99a&quot;,</span><br><span class="line">                &quot;sha256:9069f84dbbe96d4c50a656a05bbe6b6892722b0d1116a8f7fd9d274f4e991bf6&quot;,</span><br><span class="line">                &quot;sha256:f6253634dc78da2f2e3bee9c8063593f880dc35d701307f30f65553e0f50c18c&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br></pre></td></tr></table></figure>


<p>rootfs</p>
<p>bootfs</p>
<p>chroot</p>
<p>aufs</p>
<p>xfs</p>
<p>Overlay2</p>
<p>copy on write </p>
<p>Writeout </p>
<p>init-layer - non commit</p>
<img src="/posts/85a08636/image-20210203234917452.png" alt="image-20210203234917452" style="zoom:70%;">



<h2 id="如何构建docker-image"><a href="#如何构建docker-image" class="headerlink" title="如何构建docker image"></a>如何构建docker image</h2><p>步骤：</p>
<ol>
<li>编写一个dockerfile</li>
<li>docker build构建镜像</li>
<li>docker run运行镜像</li>
<li>docker push发布镜像</li>
</ol>
<h3 id="DockerFile构建过程"><a href="#DockerFile构建过程" class="headerlink" title="DockerFile构建过程"></a>DockerFile构建过程</h3><h4 id="基础知识："><a href="#基础知识：" class="headerlink" title="基础知识："></a><strong>基础知识：</strong></h4><ol>
<li>每个保留关键字都是大写</li>
<li>顺序执行</li>
<li>#表示注释</li>
<li>每一个指令都会创建提交一个新的镜像层。</li>
</ol>
<h4 id="DockerFile指令"><a href="#DockerFile指令" class="headerlink" title="DockerFile指令"></a>DockerFile指令</h4><p><img src="/posts/85a08636/image-20210128174810881-1611828531908.png" alt="image-20210128174810881"></p>
<h4 id="CMD-和-ENTRYPOINT的区别"><a href="#CMD-和-ENTRYPOINT的区别" class="headerlink" title="CMD  和 ENTRYPOINT的区别"></a>CMD  和 ENTRYPOINT的区别</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker run image-id</span> </span><br><span class="line">CMD [&quot;ls&quot;,&quot;-a&quot;]  # 指定容器启动时要运行的命令，最后一个有效，可被替代。在docker run时不能被追加参数</span><br><span class="line">ENTRYPOINT [&quot;ls&quot;, [&quot;-a&quot;]] # 指定容器启动时要运行的命令，可以在docker run的时候追加参数</span><br><span class="line"> 													# 比如 docker run xxx &quot;-l&quot;</span><br></pre></td></tr></table></figure>
<h5 id="测试CMD"><a href="#测试CMD" class="headerlink" title="测试CMD"></a>测试CMD</h5><p>写了两个CMD，在docker run的时候，发现只执行了最后一个，并没有echo出”Hello”，也不能追加参数，会报错。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vagrant@localhost dockerfile]$ cat docker-cmd-test</span><br><span class="line">FROM centos</span><br><span class="line">CMD [&quot;echo&quot;, &quot;Hello&quot;]</span><br><span class="line">CMD [&quot;ls&quot;, &quot;-a&quot;]</span><br><span class="line"></span><br><span class="line">[vagrant@localhost dockerfile]$ docker run cmdtest</span><br><span class="line">.</span><br><span class="line">..</span><br><span class="line">.dockerenv</span><br><span class="line">bin</span><br><span class="line">dev</span><br><span class="line">etc</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 尝试追加一个参数，想达到ls -al的效果，失败。</span></span><br><span class="line">[vagrant@localhost dockerfile]$ docker run cmdtest -l</span><br><span class="line">docker: Error response from daemon: OCI runtime create failed: container_linux.go:370: starting container process caused: exec: &quot;-l&quot;: executable file not found in $PATH: unknown.</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 但是可以在docker run的后面直接加命令。</span></span><br><span class="line">[vagrant@localhost dockerfile]$ docker run cmdtest ls -al</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x.   1 root root   6 Jan 30 11:26 .</span><br><span class="line">drwxr-xr-x.   1 root root   6 Jan 30 11:26 ..</span><br><span class="line">-rwxr-xr-x.   1 root root   0 Jan 30 11:26 .dockerenv</span><br><span class="line">lrwxrwxrwx.   1 root root   7 Nov  3 15:22 bin -&gt; usr/bin</span><br><span class="line">drwxr-xr-x.   5 root root 340 Jan 30 11:26 dev</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h5 id="测试ENTRYPOINT"><a href="#测试ENTRYPOINT" class="headerlink" title="测试ENTRYPOINT"></a>测试ENTRYPOINT</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vagrant@localhost dockerfile]$ cat  docker-cmd-entrypoint</span><br><span class="line">FROM centos</span><br><span class="line">ENTRYPOINT [&quot;echo&quot;,&quot;Hello&quot;]</span><br><span class="line">ENTRYPOINT [&quot;ls&quot;,&quot;-a&quot;]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 发现ENTRYPOINT也是只执行最后一个</span></span><br><span class="line">[vagrant@localhost dockerfile]$ docker run entrypoint</span><br><span class="line">.</span><br><span class="line">..</span><br><span class="line">.dockerenv</span><br><span class="line">bin</span><br><span class="line">dev</span><br><span class="line">etc</span><br><span class="line">home</span><br><span class="line">lib</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 但是ENTRYPOINT可以追加参数</span></span><br><span class="line">[vagrant@localhost dockerfile]$ docker run entrypoint -l</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x.   1 root root   6 Jan 30 11:31 .</span><br><span class="line">drwxr-xr-x.   1 root root   6 Jan 30 11:31 ..</span><br><span class="line">-rwxr-xr-x.   1 root root   0 Jan 30 11:31 .dockerenv</span><br><span class="line">lrwxrwxrwx.   1 root root   7 Nov  3 15:22 bin -&gt; usr/bin</span><br><span class="line">drwxr-xr-x.   5 root root 340 Jan 30 11:31 dev</span><br><span class="line">drwxr-xr-x.   1 root root  66 Jan 30 11:31 etc</span><br></pre></td></tr></table></figure>


<h4 id="COPY和ADD的区别"><a href="#COPY和ADD的区别" class="headerlink" title="COPY和ADD的区别"></a>COPY和ADD的区别</h4><p>add会自动解压</p>
<h5 id="COPY测试"><a href="#COPY测试" class="headerlink" title="COPY测试"></a>COPY测试</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vagrant@localhost dockerfile]$ tree .</span><br><span class="line">.</span><br><span class="line">|-- copy01</span><br><span class="line">|-- copy02</span><br><span class="line">|-- test-dir/</span><br><span class="line">    |-- sub-dir/</span><br><span class="line">    |   |-- test3</span><br><span class="line">    |-- test1</span><br><span class="line">    |-- test2</span><br><span class="line">[vagrant@localhost dockerfile]$ cat docker-copy</span><br><span class="line">FROM centos</span><br><span class="line">COPY copy01 .</span><br><span class="line">COPY copy02 /home</span><br><span class="line">COPY test-dir .  # 不会拷贝目录，只拷贝目录下的文件和目录</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果不指定WORKDIR，默认是在跟目录下面</span></span><br><span class="line">[root@e4a55c64ff8f /]# ls</span><br><span class="line">bin  copy01  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> COPY可以指定目的路径</span></span><br><span class="line">[root@e4a55c64ff8f home]# ls</span><br><span class="line">copy02</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 不会拷贝目录，只拷贝目录下的文件和目录</span></span><br><span class="line">[root@e8be2eb3f129 /]# ls</span><br><span class="line">bin  copy01  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sub-dir  sys  test1  test2  tmp  usr  var</span><br><span class="line">[root@e8be2eb3f129 /]# cd sub-dir/</span><br><span class="line">[root@e8be2eb3f129 sub-dir]# ls</span><br><span class="line">test3</span><br></pre></td></tr></table></figure>
<p>此外，COPY还可以用来实现<a href="https://www.cnblogs.com/sparkdev/p/8508435.html">multi-stage(多阶段构建)</a>，就是一个dockerfile里有多个FROM，每个 FROM 指令代表一个 stage 的开始部分。我们可以把一个 stage 的产物拷贝到另一个 stage 中。</p>
<h5 id="ADD测试"><a href="#ADD测试" class="headerlink" title="ADD测试"></a>ADD测试</h5><p>除了不能用在 multistage 的场景下，ADD 命令可以完成 COPY 命令的所有功能，并且还可以完成两类超酷的功能：</p>
<ul>
<li>解压压缩文件并把它们添加到镜像中</li>
<li>从 url 拷贝文件到镜像中</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">WORKDIR /app</span><br><span class="line">ADD nickdir.tar.gz .  </span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以把压缩包自动解压到/app目录下</span></span><br><span class="line">ADD http://example.com/big.tar.xz /usr/src/things/ </span><br><span class="line"><span class="meta">#</span><span class="bash"> 会直接把tar包加入镜像中，不会自动解压，实则增加了镜像的大小，不建议这么做！</span></span><br></pre></td></tr></table></figure>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>COPY 命令是为最基本的用法设计的，概念清晰，操作简单。而 ADD 命令基本上是 COPY 命令的超集(除了 multistage 场景)，可以实现一些方便、酷炫的拷贝操作。ADD 命令在增加了功能的同时也增加了使用它的复杂度，比如从 url 拷贝压缩文件时弊大于利。</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>浅析docker网络</title>
    <url>/posts/3e6a7048.html</url>
    <content><![CDATA[<h3 id="docker0"><a href="#docker0" class="headerlink" title="docker0"></a>docker0</h3><p>先来查看一下host的网络：</p>
<img src="/posts/3e6a7048/image-20210201164717688.png" alt="image-20210201164717688" style="zoom:50%;">



<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动一个tomcat容器</span></span><br><span class="line">[vagrant@localhost ~]$ docker run -d -P --name tomcat01 tomcat</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看容器里的网络，发现有一个 eth0@if37（172.17.0.2）和host上的docker0（172.17.0.1）在同一个网段。</span></span><br><span class="line">[vagrant@localhost dockerfile]$ docker exec -it ef59e03b0b34 ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">36: eth0@if37: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">     </span><br><span class="line"><span class="meta">#</span><span class="bash"> 从host上ping容器的eth0@if37，可以ping通。</span></span><br><span class="line">[vagrant@localhost dockerfile]$ ping 172.17.0.2</span><br><span class="line">PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.083 ms</span><br><span class="line">64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.061 ms</span><br></pre></td></tr></table></figure>
<p>我们每启动一个docker容器，docker就会给容器分配一个ip，我们只要安装的docker，就会有一个docker0网卡，这个网卡是桥接模式，使用的技术是ve th-pair技术。</p>
<p>启动完tomcat容器后，再在host上查看网络接口，会发现多了一个接口：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vagrant@localhost ~]$ ip a</span><br><span class="line">...</span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line">37: veth9f8a1b7@if36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default</span><br><span class="line">    link/ether 0a:6f:15:99:d0:b8 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet6 fe80::86f:15ff:fe99:d0b8/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<h3 id="veth-pair技术"><a href="#veth-pair技术" class="headerlink" title="veth-pair技术"></a>veth-pair技术</h3><p>是一对虚拟设备接口，都是成对出现的。</p>
<p>我们再起一个tomcat，现在有两个容器，tomcat01和tomcat02。看一下他们的网络接口和host的网络。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> -it tomcat01 ip a</span></span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">36: eth0@if37: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">       </span><br><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> -it tomcat02 ip a</span></span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">38: eth0@if39: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    </span><br><span class="line"><span class="meta">$</span><span class="bash"> ip a</span></span><br><span class="line">...</span><br><span class="line">3: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 02:42:95:5a:03:dc brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:95ff:fe5a:3dc/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">37: veth9f8a1b7@if36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default</span><br><span class="line">    link/ether 0a:6f:15:99:d0:b8 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet6 fe80::86f:15ff:fe99:d0b8/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">39: veth11e4e0f@if38: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default</span><br><span class="line">    link/ether ba:3b:43:24:29:92 brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line">    inet6 fe80::b83b:43ff:fe24:2992/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<p>网络模型如下，每个容器在创建的时候会建立一对veth pair，用于和docker0通信，不同容器间的通信也是通过docker0网桥。</p>
<img src="/posts/3e6a7048/image-20210201170031892.png" alt="image-20210201170031892" style="zoom:50%;">

<h3 id="limitation"><a href="#limitation" class="headerlink" title="limitation"></a>limitation</h3><p>测试两个容器的连通性</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ping容器名是ping不通的</span></span><br><span class="line">vagrant@localhost ~]$ docker exec -it tomcat01 ping tomcat02</span><br><span class="line">ping: tomcat02: No address associated with hostname</span><br><span class="line"><span class="meta">#</span><span class="bash"> ping ip可以ping通</span></span><br><span class="line">[vagrant@localhost ~]$ docker exec -it tomcat01 ping 172.17.0.3</span><br><span class="line">PING 172.17.0.3 (172.17.0.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 172.17.0.3: icmp_seq=1 ttl=64 time=0.058 ms</span><br><span class="line">64 bytes from 172.17.0.3: icmp_seq=2 ttl=64 time=0.066 ms</span><br></pre></td></tr></table></figure>
<p>这就是docker0网桥的局限性，<strong>不支持容器名访问</strong>。这里我们可以通过<code>--link</code>来实现通过容器名来连通。</p>
<h4 id="–link"><a href="#–link" class="headerlink" title="–link"></a>–link</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 删除tomcat02，并且重新创建</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker run -d -P --name tomcat02 --link tomcat01 tomcat</span></span><br><span class="line">a9a926b40eb97a28d39ec67418efaa05b362b9917f7375620da18e8f13607282</span><br><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> -it tomcat02 ping tomcat01</span></span><br><span class="line">PING tomcat01 (172.17.0.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from tomcat01 (172.17.0.2): icmp_seq=1 ttl=64 time=0.090 ms</span><br><span class="line">64 bytes from tomcat01 (172.17.0.2): icmp_seq=2 ttl=64 time=0.060 ms</span><br><span class="line">^C</span><br><span class="line">--- tomcat01 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 11ms</span><br><span class="line">rtt min/avg/max/mdev = 0.060/0.075/0.090/0.015 ms</span><br></pre></td></tr></table></figure>
<p>发现可以通过容器名ping通了，但是反向ping还是不通。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> -it tomcat01 ping tomcat02</span></span><br><span class="line">ping: tomcat02: No address associated with hostname</span><br></pre></td></tr></table></figure>
<p>原因是因为我们只是在docker run tomcat02的时候加了<code>--link</code>,相当于只对tomcat02设置了针对tomcat01的连通性，反之则没有，所以tomcat01并不知道如何处理tomcat02这个域名。</p>
<p>那么，<code>--link</code>  到底在tomcat02里配置了什么呢？ 下面是<code>docker inspect tomcat02</code>的输出，可以看到在HostConfig中有一个关于tomcat01的设置。</p>
<img src="/posts/3e6a7048/image-20210201171348699.png" alt="image-20210201171348699" style="zoom:50%;">

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> --link主要干了两件事</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1. 设置tomcat01（被link容器）的环境变量</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span>  -it tomcat02 env | grep -i tomcat01</span></span><br><span class="line">TOMCAT01_PORT=tcp://172.17.0.2:8080</span><br><span class="line">TOMCAT01_PORT_8080_TCP=tcp://172.17.0.2:8080</span><br><span class="line">TOMCAT01_PORT_8080_TCP_ADDR=172.17.0.2</span><br><span class="line">TOMCAT01_PORT_8080_TCP_PORT=8080</span><br><span class="line">TOMCAT01_PORT_8080_TCP_PROTO=tcp</span><br><span class="line">TOMCAT01_NAME=/tomcat02/tomcat01</span><br><span class="line">TOMCAT01_ENV_LANG=C.UTF-8</span><br><span class="line">TOMCAT01_ENV_JAVA_HOME=/usr/local/openjdk-11</span><br><span class="line">TOMCAT01_ENV_JAVA_VERSION=11.0.9.1</span><br><span class="line">TOMCAT01_ENV_CATALINA_HOME=/usr/local/tomcat</span><br><span class="line">TOMCAT01_ENV_TOMCAT_NATIVE_LIBDIR=/usr/local/tomcat/native-jni-lib</span><br><span class="line">TOMCAT01_ENV_LD_LIBRARY_PATH=/usr/local/tomcat/native-jni-lib</span><br><span class="line">TOMCAT01_ENV_GPG_KEYS=05AB33110949707C93A279E3D3EFE6B686867BA6 07E48665A34DCAFAE522E5E6266191C37C037D42 47309207D818FFD8DCD3F83F1931D684307A10A5 541FBE7D8F78B25E055DDEE13C370389288584E7 61B832AC2F1C5A90F0F9B00A1C506407564C17A3 79F7026C690BAA50B92CD8B66A3AD3F4F22C4FED 9BA44C2621385CB966EBA586F72C284D731FABEE A27677289986DB50844682F8ACB77FC2E86E29AC A9C5DF4D22E99998D9875A5110C01C5A2F6059E7 DCFD35E0BF8CA7344752DE8B6FB21E8933C60243 F3A04C595DB5B6A5F1ECA43E3B7BBB100D811BBE F7DA48BB64BCB84ECBA7EE6935CD23C10D498E23</span><br><span class="line">TOMCAT01_ENV_TOMCAT_MAJOR=9</span><br><span class="line">TOMCAT01_ENV_TOMCAT_VERSION=9.0.41</span><br><span class="line">TOMCAT01_ENV_TOMCAT_SHA512=b6450e590a37c5bccf049b1176c441f0964796995e80d4c7c7d9fb74f9ad817107c303b6b83ed3d71c9251b2b8acf334b90a4abdf9deea122e338643cece0766</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. 设置/etc/hosts</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span>  -it tomcat02 cat /etc/hosts</span></span><br><span class="line">127.0.0.1       localhost</span><br><span class="line">::1     localhost ip6-localhost ip6-loopback</span><br><span class="line">fe00::0 ip6-localnet</span><br><span class="line">ff00::0 ip6-mcastprefix</span><br><span class="line">ff02::1 ip6-allnodes</span><br><span class="line">ff02::2 ip6-allrouters</span><br><span class="line">172.17.0.2      tomcat01 ef59e03b0b34  # 这里设置了tomcat01的IP映射关系</span><br><span class="line">172.17.0.3      a9a926b40eb9</span><br></pre></td></tr></table></figure>
<h3 id="自定义网络"><a href="#自定义网络" class="headerlink" title="自定义网络"></a>自定义网络</h3><p>在docker 安装的时候，有</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker network ls</span></span><br><span class="line">NETWORK ID     NAME      DRIVER    SCOPE</span><br><span class="line">b3d482ea68aa   bridge    bridge    local  # veth pair - &gt; docker0 桥接模式（缺省模式）</span><br><span class="line">70901896d6a3   host      host      local  # 和host共享网络</span><br><span class="line">51ec536b7953   none      null      local  # 不配置网络，一般不用</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 还有一种container模式，容器内网络连通（不常用）。</span></span><br></pre></td></tr></table></figure>
<p>bridge网络里面配置了gateway， subnet，并且可以看到连接在这个网络上的容器。</p>
<p>这里的gateway就说doker0网桥（172.17.0.1）</p>
<img src="/posts/3e6a7048/image-20210201164519343.png" alt="image-20210201164519343" style="zoom:50%;">

<h4 id="创建一个新的网络"><a href="#创建一个新的网络" class="headerlink" title="创建一个新的网络"></a>创建一个新的网络</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 默认driver就是bridge</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --driver bridge</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --subnet 192.168.0.0/16 (192.168.0.1- 192.168.255.255)</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker network create -d bridge --gateway 192.168.0.1 --subnet 192.168.0.0/16 mynet</span></span><br><span class="line">49025b4d0eaa1303c112bbec9290a306fac466b9bc56aa81237d823476d86ada</span><br><span class="line"><span class="meta">$</span><span class="bash"> docker network ls</span></span><br><span class="line">NETWORK ID     NAME      DRIVER    SCOPE</span><br><span class="line">b3d482ea68aa   bridge    bridge    local</span><br><span class="line">70901896d6a3   host      host      local</span><br><span class="line">49025b4d0eaa   mynet     bridge    local</span><br><span class="line">51ec536b7953   none      null      local</span><br></pre></td></tr></table></figure>
<h4 id="在mynet网络里创建两个新的tomcat"><a href="#在mynet网络里创建两个新的tomcat" class="headerlink" title="在mynet网络里创建两个新的tomcat"></a>在mynet网络里创建两个新的tomcat</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 起两个在mynet里的tomcat</span></span><br><span class="line">[vagrant@localhost ~]$ docker run -d -P --name tomcat-net-01 --net mynet tomcat</span><br><span class="line">8151712ef76b6315018f969b6c0b8116b297a8391793736f7d7825c88a89cc8c</span><br><span class="line">[vagrant@localhost ~]$ docker run -d -P --name tomcat-net-02 --net mynet tomcat</span><br><span class="line">8dada8500e5c3c7993b649984f64c0bb8266a52d6e38256a73807a8e99d901cd</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 发现这两个容器可以通过容器名互通。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这就是自定义网络比docker默认的bridge网络（docker0）更高级的地方。</span></span><br><span class="line">[vagrant@localhost ~]$ docker exec -it tomcat-net-01 ping tomcat-net-02</span><br><span class="line">PING tomcat-net-02 (192.168.0.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=1 ttl=64 time=0.082 ms</span><br><span class="line">64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=2 ttl=64 time=0.078 ms</span><br><span class="line">^C</span><br><span class="line">--- tomcat-net-02 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 2ms</span><br><span class="line">rtt min/avg/max/mdev = 0.078/0.080/0.082/0.002 ms</span><br><span class="line">[vagrant@localhost ~]$ docker exec -it tomcat-net-02 ping tomcat-net-01</span><br><span class="line">PING tomcat-net-01 (192.168.0.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from tomcat-net-01.mynet (192.168.0.2): icmp_seq=1 ttl=64 time=0.059 ms</span><br><span class="line">64 bytes from tomcat-net-01.mynet (192.168.0.2): icmp_seq=2 ttl=64 time=0.068 ms</span><br><span class="line"></span><br><span class="line">64 bytes from tomcat-net-01.mynet (192.168.0.2): icmp_seq=3 ttl=64 time=0.216 ms</span><br><span class="line">^C</span><br><span class="line">--- tomcat-net-01 ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 90ms</span><br><span class="line">rtt min/avg/max/mdev = 0.059/0.114/0.216/0.072 ms</span><br></pre></td></tr></table></figure>
<h4 id="不同docker网络里的容器连通（跨网络）"><a href="#不同docker网络里的容器连通（跨网络）" class="headerlink" title="不同docker网络里的容器连通（跨网络）"></a>不同docker网络里的容器连通（跨网络）</h4><p>这时又有一个问题。现在我的host上的容器如图所示。</p>
<img src="/posts/3e6a7048/image-20210201173721626.png" alt="image-20210201173721626" style="zoom:50%;">

<p>tomcat01和tomcat02是在默认的docker0的网络里，tomcat-net-01和tomcat-net-02在我新创建的mynet里。如果我需要tomcat01和tomcat-net-0连通，应该怎么做呢。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker network connect mynet tomcat01</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 把tomcat容器连接到mynet网络中</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 现在就可以ping通了</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> -it tomcat01 ping tomcat-net-02</span> </span><br><span class="line">PING tomcat-net-02 (192.168.0.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=1 ttl=64 time=0.235 ms</span><br><span class="line">64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=2 ttl=64 time=0.160 ms</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 看一下tomcat01的网卡，发现多了一个eth1@if48，对应IP是 192.168.0.4，在mynet网段中。</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> -it tomcat01 ip a</span></span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">36: eth0@if37: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">47: eth1@if48: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 02:42:c0:a8:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 192.168.0.4/16 brd 192.168.255.255 scope global eth1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>


<h3 id="docker的几种网络模式"><a href="#docker的几种网络模式" class="headerlink" title="docker的几种网络模式"></a>docker的几种网络模式</h3><h4 id="host模式"><a href="#host模式" class="headerlink" title="host模式"></a>host模式</h4><p>当我们在容器中执行任何类似ifconfig命令查看网络环境时，看到的都是宿主机上的信息。而外界访问容器中的应用，则直接使用10.10.101.105:80即可，不用任何NAT转换，就如直接跑在宿主机中一样。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 和host一样，共享</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> -it bb-host ip ad</span></span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel qlen 1000</span><br><span class="line">    link/ether 52:54:00:27:8b:50 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0</span><br><span class="line">       valid_lft 44372sec preferred_lft 44372sec</span><br><span class="line">    inet6 fe80::5054:ff:fe27:8b50/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue</span><br><span class="line">    link/ether 02:42:95:5a:03:dc brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:95ff:fe5a:3dc/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">42: br-49025b4d0eaa: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue</span><br><span class="line">    link/ether 02:42:ad:cf:bc:6f brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.0.1/16 brd 192.168.255.255 scope global br-49025b4d0eaa</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:adff:fecf:bc6f/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<h4 id="container模式"><a href="#container模式" class="headerlink" title="container模式"></a>container模式</h4><p>这个模式指定新创建的容器和已经存在的一个容器共享一个Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过lo网卡设备通信。</p>
<h4 id="none模式"><a href="#none模式" class="headerlink" title="none模式"></a>none模式</h4><p>这个模式和前两个不同。在这种模式下，Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。也就是说，这个Docker容器没有网卡、IP、路由等信息。需要我们自己为Docker容器添加网卡、配置IP等。</p>
<h4 id="bridge模式"><a href="#bridge模式" class="headerlink" title="bridge模式"></a>bridge模式</h4><p>bridge模式是Docker默认的网络设置，就是docker0的模式。此模式会为每一个容器分配Network Namespace、设置IP等，并将一个主机上的Docker容器连接到一个虚拟网桥上。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><p>Docker0：默认网络，缺点是不能通过容器名访问。</p>
</li>
<li><p>–link可以解决，但是太麻烦，是单向的。</p>
</li>
<li><p>一般需要自定义网络，这个可以通过容器名访问。</p>
</li>
<li><p>如果不同网络里的容器需要互联，则需要<code>docker network connect</code>，这个命令可以把network1中的容器连接到network2中，此时容器有两个IP，分别对应network1和2。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title>容器volume基本介绍</title>
    <url>/posts/4b81a639.html</url>
    <content><![CDATA[<h2 id="容器volume"><a href="#容器volume" class="headerlink" title="容器volume"></a>容器volume</h2><h3 id="匿名挂载，具名挂载和指定路径挂载"><a href="#匿名挂载，具名挂载和指定路径挂载" class="headerlink" title="匿名挂载，具名挂载和指定路径挂载"></a>匿名挂载，具名挂载和指定路径挂载</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 匿名挂载</span></span><br><span class="line">-v 容器内路径</span><br><span class="line">docker run -d -P --name nginx01 -v /etc/nginx nginx</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看node 上的volume</span></span><br><span class="line">ubuntu@xinrantest01:~$ docker volume ls</span><br><span class="line">DRIVER              VOLUME NAME</span><br><span class="line">local               cf411242c0f1d323d2d2ab8a81d144caa6d536d992954231f426c35221adf2e3</span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以发现，匿名挂载只写容器里的路径，没有写host上的路径</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 具名挂载</span></span><br><span class="line">-v host路径：容器内路径 （与-p相似）</span><br><span class="line">ubuntu@xinrantest01:~$ docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx nginx</span><br><span class="line">5f61a3a6d01cd8db3bd99fa1738d0b6cfc74df366dc8ba8b1f4cbe0c97b2cc72</span><br><span class="line">ubuntu@xinrantest01:~$ docker volume ls</span><br><span class="line">DRIVER              VOLUME NAME</span><br><span class="line">local               cf411242c0f1d323d2d2ab8a81d144caa6d536d992954231f426c35221adf2e3</span><br><span class="line">local               juming-nginx</span><br><span class="line"><span class="meta">#</span><span class="bash">这里指定了juming-nginx为host上要挂载的目录，这个目录在docker run时被创建</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看volume在host上的路径</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/4b81a639/image-20210128164757688.png" alt="image-20210128164757688"></p>
<p><img src="/posts/4b81a639/image-20210128164837865.png" alt="image-20210128164837865"></p>
<p>所有docker 容器内的卷，没有指定目录的情况下，都是在<code>/var/lib/docker/volumes/xxxx/_data</code></p>
<p>大多数情况都是使用具名挂载，方便在host上查找volumes</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 如何确定挂载方式</span></span><br><span class="line">-v container-path    # 匿名挂载</span><br><span class="line">-v host-path：container-path # 指定路径挂载</span><br><span class="line">-v volume-name（临时起的）：container-path #具名挂载</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 限定容器的权限</span></span><br><span class="line">docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:ro   nginx</span><br><span class="line">docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:rw   nginx</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ro 这个volume只能通过host来操作，container里只有<span class="built_in">read</span> only的权限</span></span><br></pre></td></tr></table></figure>
<h3 id="用DockerFile实现volume"><a href="#用DockerFile实现volume" class="headerlink" title="用DockerFile实现volume"></a>用DockerFile实现volume</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> create a dockerfile</span></span><br><span class="line">FROM centos</span><br><span class="line"></span><br><span class="line">VOLUME [&quot;volume01&quot;,&quot;volume02&quot;&quot;]</span><br><span class="line"></span><br><span class="line">CMD echo &quot;===end===&quot;</span><br><span class="line">CMD /bin/bash</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<img src="/posts/4b81a639/image-20210128170911709.png" alt="image-20210128170911709" style="zoom:80%;">

<p>可以通过在<code>var/lib/docker/volumes/</code>路径下查看对应的目录：</p>
<p><img src="/posts/4b81a639/image-20210128171143827.png" alt="image-20210128171143827"></p>
<h3 id="数据卷容器"><a href="#数据卷容器" class="headerlink" title="数据卷容器"></a>数据卷容器</h3><p>要容器与容器之间同步数据。</p>
<img src="/posts/4b81a639/image-20210128171813726.png" alt="image-20210128171813726" style="zoom:60%;">

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动数据卷容器docker01</span></span><br><span class="line">docker run -it --name docker01 xinran/centos:1.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动多个“父容器”docker02，docker03，用--volumes-from指定数据卷容器</span></span><br><span class="line">docker run -it --name docker02  --volumes-from docker01 xinran/centos:1.0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以指定docker02或者docker01为数据卷容器，这几个容器中volumes都是同步的</span></span><br><span class="line">docker run -it --name docker03  --volumes-from docker02 xinran/centos:1.0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除docker01，也不会影响docker02，docker03的volume</span></span><br></pre></td></tr></table></figure>


<p><img src="/posts/4b81a639/image-20210128172515580.png" alt="image-20210128172515580"></p>
<p>结论：容器之间配置信息的传递，数据卷容器的生命周期一直持续到没有容器使用为止。</p>
<p>但是一旦持久话到了本地，即使删除了所有有这个volume的容器，本地的数据是不会删除的。</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>ebpf-intro</title>
    <url>/posts/85f2c2fa.html</url>
    <content><![CDATA[<p>bpf - bakerly profiling filter</p>
<p>最早是抓包用的，最经典的应用：tcpdump就用bpf。</p>
<p>没有bpf之前，抓包要抓nic上所有的包，然后再进行分析</p>
<p>有了bpf之后，可以插入一段小的代码可以指定tcp port，ip等。</p>
<p>ebpf - extension bpf</p>
<p>主要做网络，但远不止网络领域。可以抓system call，linux kernel的function call</p>
<p>kprobe -&gt; fetch kernel</p>
<p>uprobe -&gt; fetch user space</p>
<p>perf events</p>
<p>XDP/AF_XDP</p>
<p>bpf vs ebpf vm - &gt; java vm / webassembly</p>
<p>c  -&gt; 编译成ebpf backend  -&gt; kernel native??</p>
<p>程序方面：</p>
<p>bpf只有两个32bit寄存器，16个32bit内存</p>
<p>ebpf有11个64bit寄存器，512个stack。没有堆的概念，因为在ebpf里面不能new。</p>
<p><img src="/posts/85f2c2fa/image-20210423154407377.png" alt="image-20210423154407377"></p>
<p>maps：user space /kernel space shares data through maps (k-v)。</p>
<p>bpftrace vs bcc vs c program </p>
]]></content>
  </entry>
  <entry>
    <title>GPU虚拟化</title>
    <url>/posts/204fd6f0.html</url>
    <content><![CDATA[<p>目前流行的几种模式：</p>
<p>pass through</p>
<p>sriov</p>
<p>半虚拟化（mdev passthrough： Intel gvt-g, Nvidia GRID vGPU)</p>
<p>全虚拟化（VMware： vSGA）</p>
<p>学习路径</p>
<p>pci -》 pci dma -》 iommu -》 passthrough</p>
<p>内存页表 - 》 tlb</p>
<p>cache -》 内存</p>
<h1 id="PCI"><a href="#PCI" class="headerlink" title="PCI"></a>PCI</h1><p>Linux PCI设备驱动实际包括<strong>Linux PCI设备驱动</strong>和<strong>设备本身驱动</strong>两部分。</p>
<p>PCI(Periheral Component Interconnect)有三种地址空间：<strong>PCI I/O空间、PCI内存地址空间和PCI配置空间</strong>。</p>
<p>PCI I/O空间和PCI内存地址空间由设备驱动程序使用</p>
<p>而PCI配置空间由Linux PCI初始化代码使用，用于配置PCI设备，比如中断号以及I/O或内存基地址。</p>
<p>内核：遍历和配置</p>
<p>遍历顺序：从host-pci开始，遇到pci bridge就到下一级pci总线继续遍历，直到遍历完成。（深度优先？）</p>
<p>配置：PCI设备中一般都带有一些RAM和ROM 空间，通常的控制/状态寄存器和数据寄存器也往往以RAM区间的形式出现，而这些区间的地址在设备内部一般都是从0开始编址的，那么当总线上挂接了多个设备时，对这些空间的访问就会产生冲突。所以，这些地址都要先映射到系统总线上，再进一步映射到内核的虚拟地址空间。配置就是通过对PCI配置空间的寄存器进行操作从而完成地址的映射。 device physical address -&gt; system virtual address??</p>
<h1 id="DMA"><a href="#DMA" class="headerlink" title="DMA"></a>DMA</h1><p><a href="https://cloud.tencent.com/developer/article/1194593">https://cloud.tencent.com/developer/article/1194593</a></p>
<h2 id="Passthrough"><a href="#Passthrough" class="headerlink" title="Passthrough"></a>Passthrough</h2><p><strong>直通模式</strong>的实现依赖于IOMMU的功能。VTD对IOVA的地址转换使得直通设备可以在硬件层次直接使用GPA（Guest Physical Address）地址。GPU直通技术相对于任何其他设备来说，会有额外的PCI 配置空间模拟和MMIO的拦截（参见QEMU VFIO quirk机制）。比如Hypervisor或者Device Module 不会允许虚拟机对GPU硬件关键寄存器的完全的访问权限，一些高权限的操作会被直接拦截。大家或许已经意识到原来直通设备也是有MMIO模拟和拦截的。这对于我们理解GPU 半虚拟化很有帮助。</p>
<h3 id="libvirt-layer"><a href="#libvirt-layer" class="headerlink" title="libvirt layer"></a>libvirt layer</h3><p>将PCI设备直接分配给guest os时，如果不先从guest中hot-unplug该设备，将无法进行migration。</p>
<p>managed: guest启动时会自动从host上detach， guest shutdown时又重新attach回host。（implict）</p>
<p>unmanged：需要手动从host detach，之后手动re-attach回host。不然libvirt会refuse。（explict）</p>
]]></content>
  </entry>
  <entry>
    <title>hexo+github</title>
    <url>/posts/ebeabd53.html</url>
    <content><![CDATA[<h3 id="安装nodejs，git"><a href="#安装nodejs，git" class="headerlink" title="安装nodejs，git"></a>安装nodejs，git</h3><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">sudo apt <span class="keyword">install</span> npm</span><br><span class="line">sudo apt <span class="keyword">install</span> nodejs</span><br><span class="line">sudo apt <span class="keyword">install</span> git</span><br></pre></td></tr></table></figure>
<h3 id="注册github帐号，创建github-repo，命名为xxinran-github-io"><a href="#注册github帐号，创建github-repo，命名为xxinran-github-io" class="headerlink" title="注册github帐号，创建github repo，命名为xxinran.github.io"></a>注册github帐号，创建github repo，命名为<code>xxinran.github.io</code></h3><h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><figure class="highlight avrasm"><table><tr><td class="code"><pre><span class="line">sudo npm install -g hexo-<span class="keyword">cli</span></span><br></pre></td></tr></table></figure>
<p>检查各项都已经成功安装：</p>
<figure class="highlight crmsh"><table><tr><td class="code"><pre><span class="line"><span class="keyword">node</span> <span class="title">-v</span></span><br><span class="line">hexo -v</span><br></pre></td></tr></table></figure>
<h3 id="创建本地博客的repo"><a href="#创建本地博客的repo" class="headerlink" title="创建本地博客的repo"></a>创建本地博客的repo</h3><figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line">hexo <span class="keyword">init</span> blog</span><br></pre></td></tr></table></figure>
<p>运行后在当前目录下会生成一个blog目录。未来所有的博客主题，博文，配置都在这个目录里。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> blog</span><br></pre></td></tr></table></figure>
<p>会发现目录结构如下：</p>
<figure class="highlight haxe"><table><tr><td class="code"><pre><span class="line">├── _config.yml  <span class="meta"># hexo的配置文件，主题，标题，格式等都在这里指定</span></span><br><span class="line">├── db.json</span><br><span class="line">├── node_modules/</span><br><span class="line">├── <span class="keyword">package</span>.json</span><br><span class="line">├── <span class="keyword">package</span>-lock.json</span><br><span class="line">├── <span class="keyword">public</span>/</span><br><span class="line">├── scaffolds/</span><br><span class="line">├── source/ <span class="meta"># 博客的静态页面的源文件，博文的md文件就存放在这里</span></span><br><span class="line">└── themes/ <span class="meta"># 存放各种主题的源码</span></span><br></pre></td></tr></table></figure>
<p>接下来在<code>blog</code>目录下运行<code>npm install</code> 来安装hexo所需的依赖。</p>
<p>在<code>blog/source/_posts/hello-world.md</code>里有一个例子，我们先用这个例子，看看效果，运行：</p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line">hexo <span class="keyword">server</span> </span><br><span class="line">or</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure>
<p>可以看到Hexo运行在本地的4000端口，在浏览器中访问<code>localhost:4000</code>，就会看到上述hello-world的示例页面。</p>
<p>我们也可以通过 <code>hexo new first-try</code>去创建一个新的md文件，可以在这个md文件里写博文，这时在4000端口我们就可以看到新的博文。（应该是需要重新运行<code>hexo s</code>，这里没有验证）</p>
<h3 id="连接gihub和hexo"><a href="#连接gihub和hexo" class="headerlink" title="连接gihub和hexo"></a>连接gihub和hexo</h3><h4 id="上传SSH公钥"><a href="#上传SSH公钥" class="headerlink" title="上传SSH公钥"></a>上传SSH公钥</h4><figure class="highlight oxygene"><table><tr><td class="code"><pre><span class="line">ssh-keygen</span><br><span class="line"># <span class="keyword">copy</span> ssh <span class="keyword">public</span> <span class="keyword">to</span> Github Account</span><br></pre></td></tr></table></figure>
<p>输入<code>ssh -T git@github.com</code>，测试添加ssh是否成功。如果看到Hi后面是你的用户名，就说明成功了.</p>
<h4 id="在Hexo的配置文件中指定向Github-Page部署"><a href="#在Hexo的配置文件中指定向Github-Page部署" class="headerlink" title="在Hexo的配置文件中指定向Github Page部署"></a>在Hexo的配置文件中指定向Github Page部署</h4><figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">vim ～<span class="regexp">/blog/</span>_config.yml</span><br></pre></td></tr></table></figure>
<p>在deploy选项中填写Github信息， 例如：</p>
<figure class="highlight less"><table><tr><td class="code"><pre><span class="line"><span class="attribute">deploy</span>:</span><br><span class="line"><span class="attribute">type</span>: git</span><br><span class="line"><span class="attribute">repository</span>: git<span class="variable">@github</span>.<span class="attribute">com</span>:xxinran/xxinran.github.io.git</span><br><span class="line"><span class="attribute">branch</span>: main # 原来是master</span><br></pre></td></tr></table></figure>
<h4 id="生成静态文件，并部署到gihub上去。"><a href="#生成静态文件，并部署到gihub上去。" class="headerlink" title="生成静态文件，并部署到gihub上去。"></a>生成静态文件，并部署到gihub上去。</h4><figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line"># <span class="keyword">generate</span> <span class="keyword">static</span> files</span><br><span class="line">hexo <span class="keyword">generate</span></span><br><span class="line"><span class="keyword">or</span> </span><br><span class="line">hexo g</span><br><span class="line"></span><br><span class="line"># deploy to Github</span><br><span class="line">hexo deploy</span><br><span class="line"><span class="keyword">or</span></span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>
<p>这一步有可能会报错<code>Deployer not found: git</code>：</p>
<ul>
<li>需要安装<code>npm install hexo-deployer-git --save</code>。</li>
<li>需要配置gitconfig。</li>
</ul>
<h4 id="打开Github-Page验证。"><a href="#打开Github-Page验证。" class="headerlink" title="打开Github Page验证。"></a>打开<a href="https://xxinran.github.io/">Github Page</a>验证。</h4><h3 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h3><ul>
<li>每次修改完blog的配置，或者新建删除post，draft等，都需要先<code>hexo clean</code>，在重新<code>hexo g &amp; hexo d</code>。</li>
</ul>
<blockquote>
<p> 参考文档： <a href="https://hexo.io/zh-cn/docs/configuration.html">https://hexo.io/zh-cn/docs/configuration.html</a><br> <a href="https://zhuanlan.zhihu.com/p/71164003">https://zhuanlan.zhihu.com/p/71164003</a></p>
</blockquote>
<h1 id="主题配置"><a href="#主题配置" class="headerlink" title="主题配置"></a>主题配置</h1><ul>
<li>在blog目录下拉取想要应用的主题</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/iissnan/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure>
<ul>
<li><p>打开站点配置文件 _config.yml，找到 theme 字段，并将其值更改为 next。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">theme: next</span><br></pre></td></tr></table></figure></li>
<li><p>切换Schema</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scheme: Pisces</span><br></pre></td></tr></table></figure></li>
<li><p>更新github上的主题</p>
<p>改完配置之后都要运行hexo clean &amp; hexo generate &amp; hexo deploy，为了方便，可以在blog目录下加入一个新的脚本deploy.sh：</p>
<blockquote>
<p>这里涉及到 “push hexo code”, 在多设备管理中会讲到 ）</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">DIR=`dirname $0`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Generate blog</span></span><br><span class="line">echo &quot;Generate and deploy...&quot;</span><br><span class="line">hexo clean</span><br><span class="line">hexo generate &amp; hexo deploy</span><br><span class="line"><span class="meta">#</span><span class="bash"> hexo d -g</span></span><br><span class="line"></span><br><span class="line">sleep 10</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Push hexo code</span></span><br><span class="line">echo &quot;Push hexo code&quot;</span><br><span class="line">git add .</span><br><span class="line">current_date=`date &quot;+%Y-%m-%d %H:%M:%S&quot;`</span><br><span class="line">git commit -m &quot;Blog updated: $current_date&quot;</span><br><span class="line"></span><br><span class="line">sleep 2</span><br><span class="line">git push origin hexo</span><br><span class="line"></span><br><span class="line">echo &quot;=====&gt;Finish!&lt;=====&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>更新脚本权限：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod 755 deploy.sh</span><br></pre></td></tr></table></figure>
<p>以后每次推送只要运行./deploy.sh即可。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>部署istio+k8s+kiali</title>
    <url>/posts/f5beba55.html</url>
    <content><![CDATA[<h3 id="环境："><a href="#环境：" class="headerlink" title="环境："></a>环境：</h3><p>ubuntu20.04</p>
<p>kubernete 1.20</p>
<h3 id="Deploy-K8s-all-in-one-using-Kubeadm"><a href="#Deploy-K8s-all-in-one-using-Kubeadm" class="headerlink" title="Deploy K8s(all in one) using Kubeadm"></a>Deploy K8s(all in one) using Kubeadm</h3><h4 id="Install-docker"><a href="#Install-docker" class="headerlink" title="Install docker"></a>Install docker</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install docker.io</span><br><span class="line">sudo groupadd docker</span><br><span class="line">sudo usermod -aG docker <span class="variable">$&#123;USER&#125;</span>  <span class="comment"># To resolve permission errors</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># check </span></span><br><span class="line">docker ps -a</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker hub login</span></span><br><span class="line">docker login</span><br><span class="line">username: xinranwang</span><br><span class="line">password: xxxxxxXf</span><br></pre></td></tr></table></figure>
<h4 id="Install-Kubeadm"><a href="#Install-Kubeadm" class="headerlink" title="Install Kubeadm"></a>Install Kubeadm</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curl</span><br><span class="line">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -</span><br><span class="line">cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list</span><br><span class="line">deb https://apt.kubernetes.io/ kubernetes-xenial main</span><br><span class="line">EOF</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y kubelet kubeadm kubectl</span><br><span class="line">sudo apt-mark hold kubelet kubeadm kubectl</span><br></pre></td></tr></table></figure>
<h4 id="Run-kubeadm"><a href="#Run-kubeadm" class="headerlink" title="Run kubeadm"></a>Run kubeadm</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo kubeadm init --pod-network-cidr=192.168.0.0/16</span><br><span class="line"></span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure>
<h4 id="Install-Calico"><a href="#Install-Calico" class="headerlink" title="Install Calico"></a>Install Calico</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml</span><br><span class="line">kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml</span><br><span class="line">watch kubectl get pods -n calico-system</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> untaint master node</span></span><br><span class="line">kubectl taint nodes --all node-role.kubernetes.io/master-</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> check</span></span><br><span class="line">kubectl get nodes -o wide</span><br></pre></td></tr></table></figure>
<h4 id="Install-Istio"><a href="#Install-Istio" class="headerlink" title="Install Istio"></a>Install Istio</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -L https://istio.io/downloadIstio | sh -</span><br><span class="line">export PATH=&quot;$PATH:/home/ubuntu/istio-1.9.0/bin&quot;</span><br><span class="line">cd istio-1.9.0</span><br><span class="line">istioctl install </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> all pods <span class="keyword">in</span> <span class="string">&quot;default&quot;</span> ns will have the sidecar</span></span><br><span class="line">kubectl label namespace default istio-injection=enabled</span><br></pre></td></tr></table></figure>
<h4 id="Deploy-sample"><a href="#Deploy-sample" class="headerlink" title="Deploy sample"></a>Deploy sample</h4><p><a href="https://istio.io/latest/docs/setup/getting-started/#bookinfo">https://istio.io/latest/docs/setup/getting-started/#bookinfo</a></p>
<p><img src="/posts/f5beba55/image-20210303143623822.png" alt="image-20210303143623822"></p>
<p>本机上（集群内），可以通过cluster IP访问：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl 10.106.23.203:9080</span></span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">  &lt;head&gt;</span><br><span class="line">    &lt;title&gt;Simple Bookstore App&lt;/title&gt;</span><br><span class="line">&lt;meta charset=&quot;utf-8&quot;&gt;</span><br><span class="line">&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt;</span><br><span class="line">&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Latest compiled and minified CSS --&gt;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>


<h5 id="集群外访问服务"><a href="#集群外访问服务" class="headerlink" title="集群外访问服务"></a>集群外访问服务</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml</span></span><br><span class="line">gateway.networking.istio.io/bookinfo-gateway created</span><br><span class="line">virtualservice.networking.istio.io/bookinfo created</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> istioctl analyze</span></span><br><span class="line">✔ No validation issues found when analyzing namespace: default.</span><br></pre></td></tr></table></figure>
<p><img src="/posts/f5beba55/image-20210303144837821.png" alt="image-20210303144837821"></p>
<p>因为我的系统不支持external loadbalancer，所以要通过nodeport的方式暴露host的接口，才可以在本机用<code>localhost:nodeport</code>来访问。</p>
<img src="/posts/f5beba55/image-20210303144916013.png" alt="image-20210303144916013" style="zoom:80%;">

<h4 id="Install-Dashboard"><a href="#Install-Dashboard" class="headerlink" title="Install Dashboard"></a>Install Dashboard</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f samples/addons $ kubectl rollout status deployment/kiali -n istio-system</span></span><br><span class="line"></span><br><span class="line">Waiting for deployment &quot;kiali&quot; rollout to finish: 0 of 1 updated replicas are available... deployment &quot;kiali&quot; successfully rolled out</span><br></pre></td></tr></table></figure>
 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> istioctl dashboard kiali</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 因为是remote server，需要建立一个tunnel</span></span><br><span class="line">http://localhost:20001/kiali</span><br><span class="line">Failed to open browser; open http://localhost:20001/kiali in your browser.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="建立tunnel"><a href="#建立tunnel" class="headerlink" title="建立tunnel"></a>建立tunnel</h5><p><img src="/posts/f5beba55/image-20210303145356395.png" alt="image-20210303145356395"></p>
<p><img src="/posts/f5beba55/image-20210303145757231.png" alt="image-20210303145757231"></p>
<p>解释上图：把remote上的所有开放端口，都通过跳板机jumper的22端口转发到本地机器的<strong>8888</strong>端口。在本地的浏览器里设置socks proxy，将本地机器的访问全部转发到<strong>8888</strong>端口，这样就可以完成通过跳板机的端口转发。</p>
<h5 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h5><p>即使按上述设置了，还是访问不到kiali dashboard。追查到原因是：</p>
<p><code>~/istio-xxxx/samples/addons/kiali.yaml</code></p>
<p>这个文件里声明了kiali service:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kiali</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">istio-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">helm.sh/chart:</span> <span class="string">kiali-server-1.29.0</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">kiali</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/name:</span> <span class="string">kiali</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/instance:</span> <span class="string">kiali-server</span></span><br><span class="line">    <span class="attr">version:</span> <span class="string">&quot;v1.29.0&quot;</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/version:</span> <span class="string">&quot;v1.29.0&quot;</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/managed-by:</span> <span class="string">Helm</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">&quot;kiali&quot;</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kiali.io/api-spec:</span> <span class="string">https://kiali.io/api</span></span><br><span class="line">    <span class="attr">kiali.io/api-type:</span> <span class="string">rest</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">20001</span></span><br><span class="line">    <span class="attr">nodePort:</span> <span class="number">30007</span>   <span class="comment"># 增加nodeport</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http-metrics</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">9090</span></span><br><span class="line">    <span class="comment"># nodePort: 30008  # 增加nodeport</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/name:</span> <span class="string">kiali</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/instance:</span> <span class="string">kiali-server</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span> <span class="comment"># 指定service的类型是nodeport，这样可以从cluster外访问（通过hostip/locahost）</span></span><br></pre></td></tr></table></figure>


<p>可以通过<strong>remote_server_ip:nodeport</strong>访问dashboar。</p>
<p><img src="/posts/f5beba55/image-20210303151337851.png" alt="image-20210303151337851"></p>
<h4 id="卸载Uninstall"><a href="#卸载Uninstall" class="headerlink" title="卸载Uninstall"></a>卸载Uninstall</h4><p>Remove sample‘s resouces: <a href="https://istio.io/latest/docs/examples/bookinfo/#cleanup">https://istio.io/latest/docs/examples/bookinfo/#cleanup</a></p>
<p>Uninstall istio: <a href="https://istio.io/latest/docs/setup/getting-started/#uninstall">https://istio.io/latest/docs/setup/getting-started/#uninstall</a></p>
]]></content>
      <categories>
        <category>deploy</category>
      </categories>
      <tags>
        <tag>service-mesh</tag>
        <tag>deploy</tag>
      </tags>
  </entry>
  <entry>
    <title>istio开发环境搭建</title>
    <url>/posts/702f5168.html</url>
    <content><![CDATA[<p>之前搭建了istio的生产环境，这次搭建开发环境。</p>
<blockquote>
<p>参考文档: <a href="https://github.com/istio/istio/wiki/Preparing-for-Development">https://github.com/istio/istio/wiki/Preparing-for-Development</a></p>
</blockquote>
<h2 id="建立本地docker-registry"><a href="#建立本地docker-registry" class="headerlink" title="建立本地docker registry"></a>建立本地docker registry</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 把container的5000端口，映射到localhost:5000</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> mounty本地的/mnt/docker_imgs到容器里的/var/lib/registry</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 用官方的registry镜像即可</span></span><br><span class="line">docker run -d   -p 5000:5000   --restart=always   --name registry   -v /mnt/docker_imgs:/var/lib/registry   registry</span><br></pre></td></tr></table></figure>
<h3 id="安装Isito开发环境"><a href="#安装Isito开发环境" class="headerlink" title="安装Isito开发环境"></a>安装Isito开发环境</h3><h4 id="设置一些环境变量"><a href="#设置一些环境变量" class="headerlink" title="设置一些环境变量"></a>设置一些环境变量</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> User specific environment and startup programs</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This defines the docker hub to use when running integration tests and building docker images</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> eg: HUB=<span class="string">&quot;docker.io/istio&quot;</span>, HUB=<span class="string">&quot;gcr.io/istio-testing&quot;</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里的docker registry就是我们上一步设置的本地仓库</span></span><br><span class="line">export HUB=&quot;localhost:5000/xinranwang&quot;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> This defines the docker tag to use when running integration tests and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> building docker images to be your user id. You may also <span class="built_in">set</span> this variable</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this to any other legitimate docker tag.</span></span><br><span class="line">export TAG=devel</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> This defines a shortcut to change directories to <span class="variable">$HOME</span>/istio.io</span></span><br><span class="line">export ISTIO=$HOME/istio.io</span><br><span class="line"></span><br></pre></td></tr></table></figure>




<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir ~/go/src/istio.io/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/istio/istio.git</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> ~/go/src/istio.io/istio</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> make DEBUG=1 build</span></span><br><span class="line">Unable to find image &#x27;gcr.io/istio-testing/build-tools:master-2021-03-01T22-30-49&#x27; locally</span><br><span class="line">master-2021-03-01T22-30-49: Pulling from istio-testing/build-tools</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> build docker images，with HUB and TAG environment variable.</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> make DEBUG=1 docker</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> push images to registry</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 会读取环境变量，就push到本地仓库了</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> make docker.push</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以看到本地仓库里有build好的docker image</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls /mnt/docker_imgs/docker/registry/v2/repositories/xinranwang/</span></span><br><span class="line">app                   app_sidecar_debian_10      app_sidecar_ubuntu_focal   istioctl  proxyv2</span><br><span class="line">app_sidecar_centos_7  app_sidecar_debian_9       app_sidecar_ubuntu_xenial  operator</span><br><span class="line">app_sidecar_centos_8  app_sidecar_ubuntu_bionic  install-cni                pilot</span><br></pre></td></tr></table></figure>
<img src="/posts/702f5168/image-20210308154035936.png" alt="image-20210308154035936" style="zoom:80%;">

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ubuntu@otcloud-node1:~/istio-1.9.1/manifests/profiles$ istioctl install -f default-local-registry.yaml</span><br><span class="line">This will install the Istio 1.9.1  profile with [&quot;Istio core&quot; &quot;Istiod&quot; &quot;Ingress gateways&quot;] components into the cluster. Proceed? (y/N) y</span><br><span class="line">✔ Istio core installed</span><br><span class="line">✔ Istiod installed</span><br><span class="line">✔ Ingress gateways installed</span><br><span class="line">✔ Installation complete</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">k get pods -nistio-system</span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE</span><br><span class="line">istio-ingressgateway-58844f985b-mdckr   1/1     Running   0          34s</span><br><span class="line">istiod-6f846c944f-zwqxf                 1/1     Running   0          38s</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kubectl label namespace default istio-injection=enabled</span><br></pre></td></tr></table></figure>


<img src="/posts/702f5168/image-20210308164345265.png" alt="image-20210308164345265" style="zoom:80%;">





<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Environment:</span><br><span class="line">------------</span><br><span class="line">ENVOY_PORT=</span><br><span class="line">INBOUND_CAPTURE_PORT=</span><br><span class="line">ISTIO_INBOUND_INTERCEPTION_MODE=</span><br><span class="line">ISTIO_INBOUND_TPROXY_MARK=</span><br><span class="line">ISTIO_INBOUND_TPROXY_ROUTE_TABLE=</span><br><span class="line">ISTIO_INBOUND_PORTS=</span><br><span class="line">ISTIO_OUTBOUND_PORTS=</span><br><span class="line">ISTIO_LOCAL_EXCLUDE_PORTS=</span><br><span class="line">ISTIO_SERVICE_CIDR=</span><br><span class="line">ISTIO_SERVICE_EXCLUDE_CIDR=</span><br><span class="line">ISTIO_META_DNS_CAPTURE=</span><br><span class="line"></span><br><span class="line">Variables:</span><br><span class="line">----------</span><br><span class="line">PROXY_PORT=15001</span><br><span class="line">PROXY_INBOUND_CAPTURE_PORT=15006</span><br><span class="line">PROXY_TUNNEL_PORT=15008</span><br><span class="line">PROXY_UID=1337</span><br><span class="line">PROXY_GID=1337</span><br><span class="line">INBOUND_INTERCEPTION_MODE=REDIRECT</span><br><span class="line">INBOUND_TPROXY_MARK=1337</span><br><span class="line">INBOUND_TPROXY_ROUTE_TABLE=133</span><br><span class="line">INBOUND_PORTS_INCLUDE=*</span><br><span class="line">INBOUND_PORTS_EXCLUDE=15090,15021,15020</span><br><span class="line">OUTBOUND_IP_RANGES_INCLUDE=*</span><br><span class="line">OUTBOUND_IP_RANGES_EXCLUDE=</span><br><span class="line">OUTBOUND_PORTS_INCLUDE=</span><br><span class="line">OUTBOUND_PORTS_EXCLUDE=</span><br><span class="line">KUBEVIRT_INTERFACES=</span><br><span class="line">ENABLE_INBOUND_IPV6=false</span><br><span class="line">DNS_CAPTURE=false</span><br><span class="line">DNS_SERVERS=[],[]</span><br><span class="line"></span><br><span class="line">Writing following contents to rules file:  /tmp/iptables-rules-1615191401124723984.txt833457753</span><br><span class="line">* nat</span><br><span class="line">-N ISTIO_INBOUND</span><br><span class="line">-N ISTIO_REDIRECT</span><br><span class="line">-N ISTIO_IN_REDIRECT</span><br><span class="line">-N ISTIO_OUTPUT</span><br><span class="line">-A ISTIO_INBOUND -p tcp --dport 15008 -j RETURN</span><br><span class="line">-A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001</span><br><span class="line">-A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15006</span><br><span class="line">-A PREROUTING -p tcp -j ISTIO_INBOUND</span><br><span class="line">-A ISTIO_INBOUND -p tcp --dport 22 -j RETURN</span><br><span class="line">-A ISTIO_INBOUND -p tcp --dport 15090 -j RETURN</span><br><span class="line">-A ISTIO_INBOUND -p tcp --dport 15021 -j RETURN</span><br><span class="line">-A ISTIO_INBOUND -p tcp --dport 15020 -j RETURN</span><br><span class="line">-A ISTIO_INBOUND -p tcp -j ISTIO_IN_REDIRECT</span><br><span class="line">-A OUTPUT -p tcp -j ISTIO_OUTPUT</span><br><span class="line">-A ISTIO_OUTPUT -o lo -s 127.0.0.6/32 -j RETURN</span><br><span class="line">-A ISTIO_OUTPUT -o lo ! -d 127.0.0.1/32 -m owner --uid-owner 1337 -j ISTIO_IN_REDIRECT</span><br><span class="line">-A ISTIO_OUTPUT -o lo -m owner ! --uid-owner 1337 -j RETURN</span><br><span class="line">-A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN</span><br><span class="line">-A ISTIO_OUTPUT -o lo ! -d 127.0.0.1/32 -m owner --gid-owner 1337 -j ISTIO_IN_REDIRECT</span><br><span class="line">-A ISTIO_OUTPUT -o lo -m owner ! --gid-owner 1337 -j RETURN</span><br><span class="line">-A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN</span><br><span class="line">-A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN</span><br><span class="line">-A ISTIO_OUTPUT -j ISTIO_REDIRECT</span><br><span class="line">COMMIT</span><br><span class="line"></span><br><span class="line">iptables-restore --noflush /tmp/iptables-rules-1615191401124723984.txt833457753</span><br><span class="line">Writing following contents to rules file:  /tmp/ip6tables-rules-1615191401199424850.txt857904100</span><br><span class="line"></span><br><span class="line">ip6tables-restore --noflush /tmp/ip6tables-rules-1615191401199424850.txt857904100</span><br><span class="line">iptables-save </span><br><span class="line"><span class="meta">#</span><span class="bash"> Generated by iptables-save v1.6.1 on Mon Mar  8 08:16:41 2021</span></span><br><span class="line">*nat</span><br><span class="line">:PREROUTING ACCEPT [0:0]</span><br><span class="line">:INPUT ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [0:0]</span><br><span class="line">:POSTROUTING ACCEPT [0:0]</span><br><span class="line">:ISTIO_INBOUND - [0:0]</span><br><span class="line">:ISTIO_IN_REDIRECT - [0:0]</span><br><span class="line">:ISTIO_OUTPUT - [0:0]</span><br><span class="line">:ISTIO_REDIRECT - [0:0]</span><br><span class="line">-A PREROUTING -p tcp -j ISTIO_INBOUND</span><br><span class="line">-A OUTPUT -p tcp -j ISTIO_OUTPUT</span><br><span class="line">-A ISTIO_INBOUND -p tcp -m tcp --dport 15008 -j RETURN</span><br><span class="line">-A ISTIO_INBOUND -p tcp -m tcp --dport 22 -j RETURN</span><br><span class="line">-A ISTIO_INBOUND -p tcp -m tcp --dport 15090 -j RETURN</span><br><span class="line">-A ISTIO_INBOUND -p tcp -m tcp --dport 15021 -j RETURN</span><br><span class="line">-A ISTIO_INBOUND -p tcp -m tcp --dport 15020 -j RETURN</span><br><span class="line">-A ISTIO_INBOUND -p tcp -j ISTIO_IN_REDIRECT</span><br><span class="line">-A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15006</span><br><span class="line">-A ISTIO_OUTPUT -s 127.0.0.6/32 -o lo -j RETURN</span><br><span class="line">-A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -m owner --uid-owner 1337 -j ISTIO_IN_REDIRECT</span><br><span class="line">-A ISTIO_OUTPUT -o lo -m owner ! --uid-owner 1337 -j RETURN</span><br><span class="line">-A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN</span><br><span class="line">-A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -m owner --gid-owner 1337 -j ISTIO_IN_REDIRECT</span><br><span class="line">-A ISTIO_OUTPUT -o lo -m owner ! --gid-owner 1337 -j RETURN</span><br><span class="line">-A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN</span><br><span class="line">-A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN</span><br><span class="line">-A ISTIO_OUTPUT -j ISTIO_REDIRECT</span><br><span class="line">-A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001</span><br><span class="line">COMMIT</span><br><span class="line"><span class="meta">#</span><span class="bash"> Completed on Mon Mar  8 08:16:41 2021</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>deploy</category>
      </categories>
      <tags>
        <tag>deploy</tag>
        <tag>service mesh</tag>
      </tags>
  </entry>
  <entry>
    <title>istio-gateway</title>
    <url>/posts/def4f008.html</url>
    <content><![CDATA[<p><img src="/posts/def4f008/image-20210309150445057.png" alt="image-20210309150445057"></p>
<p>serviceentry：也可以访问外部服务。</p>
<h2 id="gateway简介"><a href="#gateway简介" class="headerlink" title="gateway简介"></a>gateway简介</h2><ol>
<li><p>L4-L6的负载均衡</p>
</li>
<li><p>提供对外的mtls</p>
</li>
</ol>
<p>在istio网格中，gateway可以部署任意多个，可以共用一个，也可以每个租户，namespace单独隔离。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Gateway</span> <span class="comment"># istio的一个CRD对象</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">bookinfo-gateway</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">istio:</span> <span class="string">ingressgateway</span> <span class="comment"># use istio default controller</span></span><br><span class="line">  <span class="attr">servers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span></span><br><span class="line">      <span class="attr">number:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">HTTP</span></span><br><span class="line">    <span class="attr">hosts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">bookinfo.com</span>       <span class="comment"># 允许通过外部访问的域名</span></span><br></pre></td></tr></table></figure>
<p><strong>VirtualService</strong>定义gateway L7路由，为访问bookinfo.com的http流量，提供路由匹配转发策略。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">bookinfo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">bookinfo.com</span>      <span class="comment"># 和上面的host字段一样</span></span><br><span class="line">  <span class="attr">gateways:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">bookinfo-gateway</span>  <span class="comment"># 绑定到上面的gateway</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">match:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">exact:</span> <span class="string">/productpage</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">prefix:</span> <span class="string">/static</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">exact:</span> <span class="string">/login</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">exact:</span> <span class="string">/logout</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">prefix:</span> <span class="string">/api/v1/products</span></span><br><span class="line">    <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span>   <span class="comment"># 如果上述条件之一满足，则去一个叫“productpage”的destinationrule</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">productpage</span></span><br><span class="line">        <span class="attr">port:</span></span><br><span class="line">          <span class="attr">number:</span> <span class="number">9080</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>kubernetes Ingress</strong> vs <strong>Isito gateway</strong></p>
<p><img src="/posts/def4f008/image-20210309151602156.png" alt="image-20210309151602156"></p>
<h2 id="gateway原理及实现"><a href="#gateway原理及实现" class="headerlink" title="gateway原理及实现"></a>gateway原理及实现</h2><figure class="highlight haml"><table><tr><td class="code"><pre><span class="line">pilot-agent</span><br><span class="line">-<span class="ruby"> <span class="symbol">args:</span></span></span><br><span class="line"><span class="ruby">   - proxy</span></span><br><span class="line"><span class="ruby">   - router</span></span><br><span class="line"><span class="ruby">   - --domain</span></span><br><span class="line"><span class="ruby">   - <span class="variable">$(</span>POD_NAMESPACE).svc.cluster.local</span></span><br><span class="line"><span class="ruby">   - --proxyLogLevel=warning</span></span><br><span class="line"><span class="ruby">   - --proxyComponentLogLevel=<span class="symbol">misc:</span>error</span></span><br><span class="line"><span class="ruby">   - --log_output_level=<span class="symbol">default:</span>info</span></span><br><span class="line"><span class="ruby">   - --serviceCluster</span></span><br><span class="line"><span class="ruby">   - istio-ingressgateway</span></span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>istio学习（1）-pilot</title>
    <url>/posts/a94c45db.html</url>
    <content><![CDATA[<h2 id="service-mesh（服务网格）"><a href="#service-mesh（服务网格）" class="headerlink" title="service mesh（服务网格）"></a>service mesh（服务网格）</h2><p>Service Mesh通过一个个”代理” 来为微服务转发/接收所有流量，通过控制这些代理，就可以实现服务连接，注册，发现，负载均衡，熔断，监控等等一系列服务治理相关功能，从而微服务的代码不再需要服务治理的实现，换句话说，也就是服务治理对于微服务开发者而言是透明的。如下图所示：绿色方块为微服务，蓝色方块为 service mesh 的代理，蓝色线条为服务间通讯。可以看到蓝色的方块和线条组成了整个网格。这个网络就是 Service Mesh。</p>
<img src="/posts/a94c45db/v2-638a9d12e8a7406b7a733f2eadf989f5_1440w.jpg" alt="img" style="zoom:30%;">

<h2 id="Istio"><a href="#Istio" class="headerlink" title="Istio"></a>Istio</h2><p>Go编写，控制面。对应的数据面是通常是envoy（基于C++）</p>
<h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><p><img src="/posts/a94c45db/arch.svg" alt="The overall architecture of an Istio-based application."></p>
<h3 id="基本组件"><a href="#基本组件" class="headerlink" title="基本组件"></a>基本组件</h3><h4 id="envoy"><a href="#envoy" class="headerlink" title="envoy"></a>envoy</h4><p>以sidecar的形式运行在服务所在的pod中，所有service的流量进出都必须通过envoy proxy，用来控制service之间的访问，service是感知不到envoy proxy的存在。</p>
<p>envoy可以和控制平面（istio的组件）双向交互，可以从istio拿到一些策略和服务发现的信息，同时也可以给istio传回一些监控的数据等等。</p>
<h4 id="Istiod"><a href="#Istiod" class="headerlink" title="Istiod"></a>Istiod</h4><p>Istiod提供服务发现, 服务配置和证书管理。</p>
<p>stiod acts as a Certificate Authority (CA) and generates certificates to allow secure mTLS communication in the data plane.</p>
<h2 id="Istio的服务发现流程"><a href="#Istio的服务发现流程" class="headerlink" title="Istio的服务发现流程"></a>Istio的服务发现流程</h2><p><img src="/posts/a94c45db/image-20210226160141651.png" alt="image-20210226160141651"></p>
<p>istio的pilot组件时负责服务发现的。在pilot里只有服务发现的定义，但是没有服务发现的实现，服务发现的实现是在pilot adapter对接的platform（比如k8s）里的。pilot adapter可以对接不同的平台，比如cloudfoudary，eurake，kubernetes，consul等等，但是k8s的适配最好，也是istio社区的重点。</p>
<h3 id="isito和kubernetes服务概念对应关系"><a href="#isito和kubernetes服务概念对应关系" class="headerlink" title="isito和kubernetes服务概念对应关系"></a>isito和kubernetes服务概念对应关系</h3><table>
<thead>
<tr>
<th align="center">isito</th>
<th align="center">kubernetes</th>
</tr>
</thead>
<tbody><tr>
<td align="center">service</td>
<td align="center">service</td>
</tr>
<tr>
<td align="center">instance</td>
<td align="center">endpoint</td>
</tr>
<tr>
<td align="center">version</td>
<td align="center">deployment</td>
</tr>
</tbody></table>
<img src="/posts/a94c45db/image-20210309105852832.png" alt="image-20210309105852832" style="zoom:80%;">

<p>每个envoy里都有全量的规则。（现在也是吗？？）</p>
<p>Envoy （C++），CNCF毕业（kubernets，Prometheus之后）。</p>
<p>xDS：</p>
<p>listener（LDS）：</p>
<p>routes（RDS）：</p>
<p>cluster（CDS）：</p>
<p>endpoint（EDS）：</p>
<p>Envoy目前支持的三种LB：</p>
<ol>
<li>轮询round robin</li>
<li>随机random</li>
<li>加权weighted least request.</li>
</ol>
<p>Istio：</p>
<p>用户可以配置（通过rules API）的CRD：</p>
<p>gateway：</p>
<p>virtualservice：</p>
<p>destinationrule：</p>
<p>serviceEntry：</p>
<h3 id="Pilot"><a href="#Pilot" class="headerlink" title="Pilot"></a>Pilot</h3><p><img src="/posts/a94c45db/image-20210423095618162.png" alt="image-20210423095618162"></p>
<ol>
<li>平台适配层: 将不同的平台资源和istio的抽象模型相互转换。</li>
<li>xds API： envoy API</li>
</ol>
<h3 id="citadel："><a href="#citadel：" class="headerlink" title="citadel："></a>citadel：</h3><p>没有citadel的时候，就是end to end的非对称密钥交换，然后对称加密传输数据。</p>
<p>citadel：就是存储证书，发放证书的一个东西。</p>
<h3 id="Galley"><a href="#Galley" class="headerlink" title="Galley"></a>Galley</h3><p>istio自身创建的CRD有50多种，可以理解成验证，解析，管理CRD的东西。</p>
<h3 id="Mixer"><a href="#Mixer" class="headerlink" title="Mixer"></a>Mixer</h3><p>1.7之后弃用，下放到envoy里。telemetry v2是在istio里？</p>
<h3 id="其他解决方案"><a href="#其他解决方案" class="headerlink" title="其他解决方案"></a>其他解决方案</h3><p>sofamesh蚂蚁金服，用<strong>mosn</strong>替换envoy，golang，合并mixer，增强pilot，增加对sofa rpc， dubbo的支持。</p>
<p>linkerd =  control plane + data plane</p>
<p>linkerd v1可以脱离k8s</p>
<p>linkerd v2和k8s紧耦合</p>
<p>consul</p>
<p>192.168.0.195 zhang</p>
<p>192.168.0.196 ci</p>
<p>192.168.0.199 zhang</p>
]]></content>
      <categories>
        <category>service-mesh</category>
      </categories>
      <tags>
        <tag>service-mesh</tag>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title>linux内存管理</title>
    <url>/posts/d0683ba8.html</url>
    <content><![CDATA[<h1 id="主流存储设备"><a href="#主流存储设备" class="headerlink" title="主流存储设备"></a>主流存储设备</h1><p><img src="/posts/d0683ba8/%E5%AD%98%E5%82%A8%20(1).svg"></p>
<h1 id="物理内存"><a href="#物理内存" class="headerlink" title="物理内存"></a>物理内存</h1><p>我们不讨论cache，flash，rom这些，只讨论memory也就是通常所说的主存，现在一般是DDR4.</p>
<img src="/posts/d0683ba8/image-20210428163238359.png" alt="image-20210428163238359" style="zoom:67%;">

<p>内存工作频率 = min(cpu外部频率，主板频率，内存自身频率)</p>
<a id="more"></a>

<p>对于一个32位系统来说，地址总线最多是32bit，它能寻址的最大范围是2^32=4GB，所以也就能插4GB的内存条，插8GB的也没有用。</p>
<p>64位CPU出现之后，其地址总线位宽一般采用的是36位或者40位，它们寻址的物理地址空间为64GB或者1T（intel）。</p>
<blockquote>
<p>注意： 常说的<strong>CPU位宽</strong>指的是<strong>数据总线</strong>位宽，最大寻址范围和CPU位宽无关。</p>
</blockquote>
<p><img src="/posts/d0683ba8/image-20210428174116842.png" alt="image-20210428174116842"></p>
<p>查询系统地址总线位宽：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo  | grep address</span><br><span class="line">address sizes   : 46 bits physical, 48 bits virtual</span><br><span class="line"><span class="meta">#</span><span class="bash"> 物理内存寻址范围/虚拟内存寻址范围可以不同</span></span><br></pre></td></tr></table></figure>
<h2 id="物理内存3个管理区"><a href="#物理内存3个管理区" class="headerlink" title="物理内存3个管理区"></a>物理内存3个管理区</h2><p><code>Linux</code> 内核会将物理内存分为3个管理区，分别是：</p>
<h3 id="ZONE-DMA"><a href="#ZONE-DMA" class="headerlink" title="ZONE_DMA"></a>ZONE_DMA</h3><p><code>DMA</code>内存区域。包含0MB~16MB之间的内存页框，可以由老式基于<code>ISA</code>的设备通过<code>DMA</code>使用，直接映射到内核的地址空间。</p>
<h3 id="ZONE-NORMAL"><a href="#ZONE-NORMAL" class="headerlink" title="ZONE_NORMAL"></a>ZONE_NORMAL</h3><p>普通内存区域。包含16MB~896MB之间的内存页框，常规页框，直接映射到内核的地址空间。</p>
<h3 id="ZONE-HIGHMEM"><a href="#ZONE-HIGHMEM" class="headerlink" title="ZONE_HIGHMEM"></a>ZONE_HIGHMEM</h3><p>高端内存区域。包含896MB以上的内存页框，不进行直接映射，可以通过永久映射和临时映射进行这部分内存页框的访问。</p>
<p><img src="/posts/d0683ba8/2.png" alt="Image"></p>
<h1 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h1><p>对于32位系统来说，为了充分利用和管理系统内存资源，Linux采用虚拟内存管理技术，利用虚拟内存技术让每个进程都有<code>4GB</code> 互不干涉的虚拟地址空间。</p>
<p>为什么是4GB? </p>
<p>之所以是 4GB ，是因为在 32 位的操作系统中，一个指针长度是 4 字节 （32位）， 2的32次 方个地址寻址能力是从 0x00000000~0xFFFFFFFF 即为 4GB 大小的容量。</p>
<p>进程初始化分配和操作的都是基于这个「虚拟地址」，只有当进程需要实际访问内存资源的时候才会建立<strong>虚拟地址和物理地址的映射</strong>，调入物理内存页。</p>
<p><img src="/posts/d0683ba8/1.png" alt="Image"></p>
<p>这么做的目的是：</p>
<ul>
<li>避免用户直接访问物理内存地址，防止一些破坏性操作，保护操作系统</li>
<li>每个进程都被分配了4GB的虚拟内存，用户程序可使用比实际物理内存更大的地址空间</li>
</ul>
<p>当需进程要实际访问内存的时候，会由内核的「请求分页机制」产生「缺页异常」调入物理内存页。</p>
<p><strong>64位系统对比：</strong></p>
<p><img src="/posts/d0683ba8/64bit.png" alt="Linux的进程地址空间[一]"></p>
<h2 id="用户态和内核态"><a href="#用户态和内核态" class="headerlink" title="用户态和内核态"></a>用户态和内核态</h2><p>用户态切换到内核态的 3 种方式：系统调用、异常、外设中断</p>
<p>用户态的程序就不能随意操作内核地址空间，具有一定的安全保护作用；内核态线程共享内核地址空间；</p>
<p><img src="/posts/d0683ba8/image-20210426162841923.png" alt="image-20210426162841923"></p>
<h2 id="用户空间虚拟内存-3GB"><a href="#用户空间虚拟内存-3GB" class="headerlink" title="用户空间虚拟内存 - 3GB"></a>用户空间虚拟内存 - 3GB</h2><p>用户进程能访问的是「用户空间」，每个进程都有自己独立的用户空间，虚拟地址范围从从 <code>0x00000000</code> 至 <code>0xBFFFFFFF</code> 总容量3G 。</p>
<p>进程（执行的程序）占用的用户空间按照「 访问属性一致的地址空间存放在一起 」的原则，划分成 <code>5</code>个不同的内存区域。访问属性指的是“可读、可写、可执行等 。</p>
<p><img src="/posts/d0683ba8/3.png" alt="Image"></p>
<ul>
<li><p>代码段</p>
<p>代码段是用来存放可执行文件的操作指令，可执行程序在内存中的镜像。代码段需要防止在运行时被非法修改，所以<strong>只准许读取操作，它是不可写的</strong>。</p>
</li>
<li><p>数据段</p>
<p>数据段用来存放可执行文件中<strong>已初始化全局变量</strong>，换句话说就是存放程序静态分配的变量和全局变量。</p>
</li>
<li><p>BSS段</p>
<p><code>BSS</code>段包含了程序中<strong>未初始化的全局变量</strong>，在内存中 <code>bss</code> 段全部置零。</p>
</li>
<li><p>堆 <code>heap</code></p>
<p>堆是用于存放进程运行中被<strong>动态分配的内存段</strong>，它的大小并不固定，可动态扩张或缩减。当进程调用malloc等函数分配内存时，新分配的内存就被动态添加到堆上（堆被扩张）；当利用free等函数释放内存时，被释放的内存从堆中被剔除（堆被缩减）</p>
</li>
<li><p>栈 <code>stack</code></p>
<p>栈是用户存放程序<strong>临时创建的局部变量</strong>，也就是函数中定义的变量（但不包括 <code>static</code> 声明的变量，static意味着在数据段中存放变量）。除此以外，在函数被调用时，其参数也会被压入发起调用的进程栈中，并且待到调用结束后，函数的返回值也会被存放回栈中。由于栈的先进先出特点，所以栈特别方便用来保存/恢复调用现场。从这个意义上讲，我们可以把堆栈看成一个寄存、交换临时数据的内存区。</p>
</li>
</ul>
<h2 id="内核空间虚拟内存-1GB"><a href="#内核空间虚拟内存-1GB" class="headerlink" title="内核空间虚拟内存 - 1GB"></a>内核空间虚拟内存 - 1GB</h2><p>在 <code>x86 32</code> 位系统里，Linux 内核地址空间是指虚拟地址从 <code>0xC0000000</code> 开始到 <code>0xFFFFFFFF</code> 为止的高端内存地址空间，总计 <code>1G</code> 的容量， 包括了内核镜像、物理页面表、驱动程序等运行在内核空间 。虚拟空间的1GB怎么对应到物理内存的4GB甚至更多（取决于地址总线最大寻址和插的内存条大小）呢？</p>
<img src="/posts/d0683ba8/image-20210429173652156.png" alt="image-20210429173652156" style="zoom:80%;">

<ul>
<li><p>直接映射区：线性空间中从 3G 开始到3GB+896M 的区间，直接映射到物理内存的 0 - 896MB（dma zone + normal zone），物理和虚拟中间差一个固定的偏移量：PAGE_OFFSET = 0xC0000000。</p>
<p>虚拟地址 = <code>PAGE_OFFSET</code> + 物理地址，也可以用 <code>virt_to_phys()</code>函数将内核虚拟空间中的线性地址转化为物理地址。</p>
</li>
<li><p>除了直接映射区：还有4G-(3G+896MB) = 128MB的空间要映射到物理内存的其他空间，显然1：1map是不够的。所以内核空间拿出了最后的 128M 地址区间，划分成下面三个高端内存映射区（动态分配，可以重复映射不同的地址）：</p>
<ul>
<li>动态内存映射区：该区域由内核函数 vmalloc 来分配，特点是：线性空间连续，但是对应的物理地址空间不一定连续。<code>vmalloc</code> 分配的线性地址所对应的物理页可能处于低端内存，也可能处于高端内存。</li>
<li>永久内存映射区：该区域可访问高端内存（highmem），访问方法是使用 <code>alloc_page (_GFP_HIGHMEM)</code> 分配高端内存页或者使用<code>kmap</code>函数将分配到的高端内存映射到该区域。</li>
<li>固定映射区：该区域和 4G 的顶端只有 4k 的隔离带，其每个地址项都服务于特定的用途，如：ACPI_BASE 等</li>
</ul>
</li>
</ul>
<h2 id="虚拟地址-物理地址的映射关系图"><a href="#虚拟地址-物理地址的映射关系图" class="headerlink" title="虚拟地址 -  物理地址的映射关系图"></a>虚拟地址 -  物理地址的映射关系图</h2><p><img src="/posts/d0683ba8/image-20210429180241789.png" alt="image-20210429180241789"></p>
<h1 id="MMU"><a href="#MMU" class="headerlink" title="MMU"></a>MMU</h1><p>把虚拟地址转换成内存的物理地址，这中间涉及利用<code>MMU</code> 内存管理单元（Memory Management Unit ) 对虚拟地址分段和分页（段页式）地址转换。</p>
<ul>
<li>MMU 是一种硬件电路，它包含两个部件，一个是分段部件，一个是分页部件</li>
<li>分段机制把一个逻辑地址转换为线性地址</li>
<li>分页机制把一个线性地址转换为物理地址</li>
</ul>
<blockquote>
<p>单片机是没有mmu的，程序直接访问物理地址。</p>
</blockquote>
<p><img src="/posts/d0683ba8/image-20210427151435405.png" alt="image-20210427151435405"></p>
<h2 id="分段机制"><a href="#分段机制" class="headerlink" title="分段机制"></a>分段机制</h2><h2 id="分页机制"><a href="#分页机制" class="headerlink" title="分页机制"></a>分页机制</h2><p>页表保存在<strong>物理内存中</strong>，<strong>MMU会查找页表来确定一个VA应该映射到什么PA。</strong></p>
<p><strong>段错误</strong>我们已经遇到过很多次了，它是这样产生的：</p>
<ol>
<li>用户程序要访问的一个VA，经MMU检查无权访问。</li>
<li>MMU产生一个异常，CPU从用户模式切换到特权模式，跳转到内核代码中执行异常服务程序。</li>
<li>内核把这个异常解释为段错误，把引发异常的进程终止掉。</li>
</ol>
<p>page -  4KB</p>
<p>struct-page </p>
<p>4G内存一共有4GB/4KB = 2 ^20个 </p>
<h2 id="物理内存分配"><a href="#物理内存分配" class="headerlink" title="物理内存分配"></a>物理内存分配</h2><h3 id="外部碎片"><a href="#外部碎片" class="headerlink" title="外部碎片"></a>外部碎片</h3><p>当需要分配大块内存的时候，要用好几页组合起来才够，而系统分配物理内存页的时候会尽量分配连续的内存页面，频繁的分配与回收物理页导致大量的小块内存夹杂在已分配页面中间，形成外部碎片，举个例子：</p>
<p><img src="/posts/d0683ba8/waibu.png" alt="Image"></p>
<h3 id="内部碎片"><a href="#内部碎片" class="headerlink" title="内部碎片"></a>内部碎片</h3><p>物理内存是按页来分配的，这样当实际只需要很小内存的时候，也会分配至少是 4K 大小的页面，而内核中有很多需要以字节为单位分配内存的场景，这样本来只想要几个字节而已却不得不分配一页内存，除去用掉的字节剩下的就形成了内部碎片。</p>
<p><img src="/posts/d0683ba8/neibu.pnf" alt="Image"></p>
<h3 id="Buddy（伙伴）分配算法-gt-外部碎片"><a href="#Buddy（伙伴）分配算法-gt-外部碎片" class="headerlink" title="Buddy（伙伴）分配算法 -&gt; 外部碎片"></a>Buddy（伙伴）分配算法 -&gt; 外部碎片</h3><p>相同大小的页框块用链表串起来，页框块就像手拉手的好伙伴，也是这个算法名字的由来。</p>
<h3 id="slab分配器-gt-内部碎片"><a href="#slab分配器-gt-内部碎片" class="headerlink" title="slab分配器 - &gt; 内部碎片"></a>slab分配器 - &gt; 内部碎片</h3><p>通过将内存按使用对象不同<strong>再划分</strong>成不同大小的空间，应用于内核对象的缓存。</p>
<h1 id="NUMA"><a href="#NUMA" class="headerlink" title="NUMA"></a>NUMA</h1><p>Node，Zone，Page</p>
<p><img src="/posts/d0683ba8/numa.jgp" alt="uma-numa"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat /proc/zoneinfo | grep Node</span><br><span class="line">Node 0, zone      DMA</span><br><span class="line">Node 0, zone    DMA32 # 64 bit system</span><br><span class="line">Node 0, zone   Normal</span><br><span class="line">Node 0, zone  Movable # 防止内存碎片化的ZONE_MOVABLE</span><br><span class="line">Node 0, zone   Device # 支持设备热插拔的ZONE_DEVICE</span><br><span class="line">Node 1, zone      DMA</span><br><span class="line">Node 1, zone    DMA32</span><br><span class="line">Node 1, zone   Normal</span><br><span class="line">Node 1, zone  Movable</span><br><span class="line">Node 1, zone   Device</span><br><span class="line"><span class="meta">#</span><span class="bash"> 没有 zone highmem， 因为是64位系统，可以直接映射</span></span><br></pre></td></tr></table></figure>









<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> lscpu</span></span><br><span class="line">Architecture:                    x86_64</span><br><span class="line">CPU op-mode(s):                  32-bit, 64-bit</span><br><span class="line">Byte Order:                      Little Endian</span><br><span class="line">Address sizes:                   46 bits physical, 48 bits virtual</span><br><span class="line">CPU(s):                          32  # cores per socket * sockets * thread per core </span><br><span class="line">On-line CPU(s) list:             0-31</span><br><span class="line">Thread(s) per core:              2</span><br><span class="line">Core(s) per socket:              8</span><br><span class="line">Socket(s):                       2</span><br><span class="line">NUMA node(s):                    2</span><br><span class="line">Vendor ID:                       GenuineIntel</span><br><span class="line">CPU family:                      6</span><br><span class="line">Model:                           45</span><br><span class="line">Model name:                      Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz</span><br><span class="line">Stepping:                        7</span><br><span class="line">CPU MHz:                         1207.495</span><br><span class="line">CPU max MHz:                     3300.0000</span><br><span class="line">CPU min MHz:                     1200.0000</span><br><span class="line">BogoMIPS:                        5187.02</span><br><span class="line">Virtualization:                  VT-x</span><br><span class="line">L1d cache:                       512 KiB  # instruction cache</span><br><span class="line">L1i cache:                       512 KiB  # data cache</span><br><span class="line">L2 cache:                        4 MiB</span><br><span class="line">L3 cache:                        40 MiB</span><br><span class="line">NUMA node0 CPU(s):               0-7,16-23</span><br><span class="line">NUMA node1 CPU(s):               8-15,24-31</span><br><span class="line">Vulnerability Itlb multihit:     KVM: Mitigation: Split huge pages</span><br><span class="line">Vulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable</span><br><span class="line">Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable</span><br><span class="line">Vulnerability Meltdown:          Mitigation; PTI</span><br><span class="line">Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp</span><br><span class="line">Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization</span><br><span class="line">Vulnerability Spectre v2:        Mitigation; Full generic retpoline, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling</span><br><span class="line">Vulnerability Srbds:             Not affected</span><br><span class="line">Vulnerability Tsx async abort:   Not affected</span><br><span class="line">Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxs</span><br><span class="line">                                 r sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good</span><br><span class="line">                                  nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2</span><br><span class="line">                                  ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx lahf</span><br><span class="line">                                 _lm epb pti ssbd ibrs ibpb stib</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/posts/d0683ba8/image-20210429154000972.png" alt="image-20210429154000972"></p>
<img src="/posts/d0683ba8/image-20210429154341590.png" alt="image-20210429154341590" style="zoom:67%;">

<h3 id><a href="#" class="headerlink" title></a></h3>]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>markdown基本语法</title>
    <url>/posts/47caaf1e.html</url>
    <content><![CDATA[<h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级"><a href="#二级" class="headerlink" title="二级"></a>二级</h2><h3 id="三级-（CTRL-3-或者-）"><a href="#三级-（CTRL-3-或者-）" class="headerlink" title="三级 （CTRL +3 或者 ### ）"></a>三级 （CTRL +3 或者 ### ）</h3><h4 id="四级"><a href="#四级" class="headerlink" title="四级"></a>四级</h4><h6 id="最多支持到六级"><a href="#最多支持到六级" class="headerlink" title="最多支持到六级"></a>最多支持到六级</h6><h2 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h2><p><strong>hello world</strong></p>
<p>CTRL + B 或者 两边加**</p>
<p><em>hello world</em></p>
<p>CTRL + I 或者两边加* </p>
<p><em><strong>hello world</strong></em></p>
<p>CTRL + B + I 或者 两边加***</p>
<p><del>hello world</del></p>
<p>ALT + SHIFT + 5  或者~~ </p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><blockquote>
<p>这是一个引用， 用&gt;开始 </p>
</blockquote>
<h2 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h2><hr>
<hr>
<p>三个-， 或者三个*</p>
<h2 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h2><p><img src="https://www.w3schools.com/images/w3schools_green.jpg" alt="图片名字"></p>
<p>感叹号[图片名字]+ (图片地址)</p>
<h2 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h2><p><a href="https://kubernetes.io/docs/concepts/services-networking/service/#ips-and-vips">超级链接</a></p>
<p>方括号里面写链接名字，后面加()写链接地址。</p>
<h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><h3 id="有序列表"><a href="#有序列表" class="headerlink" title="有序列表"></a>有序列表</h3><ol>
<li>A</li>
<li>B</li>
<li>C</li>
</ol>
<h3 id="无序列表"><a href="#无序列表" class="headerlink" title="无序列表"></a>无序列表</h3><p>减号 + space</p>
<ul>
<li>A</li>
<li>B</li>
</ul>
<p>CTRL + [  回到上一级<br>CTRL + ]  回到下一级</p>
<h2 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h2><table>
<thead>
<tr>
<th>名字</th>
<th>性别</th>
<th>生日</th>
</tr>
</thead>
<tbody><tr>
<td>张三</td>
<td>男</td>
<td>1997.1</td>
</tr>
</tbody></table>
<p>名字 | 性别| 生日|</p>
<p>–|–|–</p>
<p>xx|xx|xx|</p>
<p>进入源代码模式(CTRL + /)，删除行间空行，就可以生成表格了。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">public</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>language</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>多设备同步hexoy</title>
    <url>/posts/c9d1870c.html</url>
    <content><![CDATA[<h1 id="多个设备之间同步hexo"><a href="#多个设备之间同步hexo" class="headerlink" title="多个设备之间同步hexo"></a>多个设备之间同步hexo</h1><p><strong>原理</strong></p>
<p>hexo deploy会将本地创建好的目录blog里面的文件生成对应的静态文件（html等），并且将它上传到xxx.github.com repo中。<br>在_config.yml中设置了deploy要操作的repo和branch，这里我使用的是xxinran.github.io的main branch。</p>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>容器的namespace和cgroups</title>
    <url>/posts/b56963ac.html</url>
    <content><![CDATA[<h2 id="namespace"><a href="#namespace" class="headerlink" title="namespace"></a>namespace</h2><h4 id="PID-namespace"><a href="#PID-namespace" class="headerlink" title="PID namespace"></a>PID namespace</h4><p>一个容器就是是一个进程，在启动下面的容器的时候：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker run -it busybox sh  <span class="comment"># sh是容器的第一个进程</span></span></span><br><span class="line">/ # ps</span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">    1 root      0:00 sh #PID=1</span><br><span class="line">    7 root      0:00 ps #PID=7</span><br></pre></td></tr></table></figure>
<p>Linux在创建新进程的时候，它的系统调用是</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> pid = clone(main_function, stack_size, SIGCHLD, <span class="literal">NULL</span>); </span><br></pre></td></tr></table></figure>
<p>我们可以在系统调用的时候，传入一个<code>CLONE_NEWPID</code>参数。这样这个进程就是在一个全新的namespace里，塔是1号进程。上述的<code>docker run</code>命令的底层，实则也是调用了：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, <span class="literal">NULL</span>); </span><br></pre></td></tr></table></figure>
<p>这只是Linux提供的namespace的一种，叫做PID namespace。</p>
<p>在新的命名空间里创建的进程，在命名空间里的PID=1，但是在host上查看，PID就不等于1了。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ps -aux | grep sh</span></span><br><span class="line">root       36046  0.0  0.0   1324     4 pts/0    Ss+  01:15   0:00 sh  # PID=36046</span><br></pre></td></tr></table></figure>
<p>此外，Linux 操作系统还提供了 <strong>Mount</strong>、<strong>UTS</strong>、<strong>IPC</strong>、<strong>Network</strong> 和 <strong>User</strong> 这些 Namespace，用来对各种不同的进程上下文进行“障眼法”操作。</p>
<h4 id="Mount-namespace"><a href="#Mount-namespace" class="headerlink" title="Mount namespace"></a>Mount namespace</h4><p>用于让被隔离进程只看到当前 Namespace 里的挂载点信息</p>
<h4 id="Network-Namespace"><a href="#Network-Namespace" class="headerlink" title="Network Namespace"></a>Network Namespace</h4><p>用于让被隔离进程看到当前 Namespace 里的网络设备和配置。</p>
<h2 id="虚拟机-vs-容器"><a href="#虚拟机-vs-容器" class="headerlink" title="虚拟机 vs 容器"></a>虚拟机 vs 容器</h2><h4 id="hypervisor和docker-engine"><a href="#hypervisor和docker-engine" class="headerlink" title="hypervisor和docker engine"></a>hypervisor和docker engine</h4><img src="/posts/b56963ac/image-20210203222905761.png" alt="image-20210203222905761" style="zoom:50%;">

<p><strong>hypervisor</strong>：对应用进程的隔离环境负责</p>
<p><strong>docker engine</strong>：对隔离性不负责任，真正对隔离性负责的是linux os（namespace）。</p>
<h4 id="VM和container"><a href="#VM和container" class="headerlink" title="VM和container"></a>VM和container</h4><table>
<thead>
<tr>
<th>VM</th>
<th>Container</th>
</tr>
</thead>
<tbody><tr>
<td>性能损耗比较大，一个VM自己就要占100-200MB。在做系统调用时，还需要经过hypervisor层的拦截和处理，这又是一层性能损耗</td>
<td>没有因为虚拟化带来的性能损耗，也没有guest os，除了容器本身之外，其他资源占用可以几乎不计。</td>
</tr>
<tr>
<td>隔离性高，不共享内核，linux集群可以起Window VM，反之亦然</td>
<td>隔离的不彻底，共享内核，不能在linux上起window容器，反之亦然</td>
</tr>
<tr>
<td>对用户要求低</td>
<td>对用户要求高，用户要知道，在容器里，什么能做，什么不能做，不然有可能改了容器里的东西，host上的东西也变了</td>
</tr>
</tbody></table>
<p><strong>例子</strong>：在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：<strong>时间</strong>。这就意味着，如果你的容器中的程序使用 settimeofday(2) 系统调用修改了时间，整个宿主机的时间都会被随之修改，这显然不符合用户的预期。</p>
<h2 id="Cgroups"><a href="#Cgroups" class="headerlink" title="Cgroups"></a>Cgroups</h2><p>知道了namespace技术之后，我们知道一个容器就是一个进程，和host上其他进程一样。那么如果这个容器进程占用了host上的所有资源，那么这个host就卡死了。这显然是不合理的。我们需要限制容器的资源上限，这就是cgroups做的事情。</p>
<p> <strong>Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。</strong>主要限制CPU，内存，磁盘，网络带宽等等。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cgroups给用户暴露出来的操作接口是文件系统</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ll /sys/fs/cgroup/</span></span><br><span class="line">total 0</span><br><span class="line">dr-xr-xr-x. 6 root root  0 Jan 30 10:16 blkio  # 为块设备制定IO限制</span><br><span class="line">lrwxrwxrwx. 1 root root 11 Jan 30 10:16 cpu -&gt; cpu,cpuacct  # cpu限制</span><br><span class="line">dr-xr-xr-x. 6 root root  0 Jan 30 10:16 cpu,cpuacct</span><br><span class="line">lrwxrwxrwx. 1 root root 11 Jan 30 10:16 cpuacct -&gt; cpu,cpuacct</span><br><span class="line">dr-xr-xr-x. 3 root root  0 Jan 30 10:16 cpuset  # 为进程分配单独的cpu核和对应的内存节点</span><br><span class="line">dr-xr-xr-x. 6 root root  0 Jan 30 10:16 devices</span><br><span class="line">dr-xr-xr-x. 3 root root  0 Jan 30 10:16 freezer</span><br><span class="line">dr-xr-xr-x. 3 root root  0 Jan 30 10:16 hugetlb</span><br><span class="line">dr-xr-xr-x. 6 root root  0 Jan 30 10:16 memory  # 内存限制</span><br><span class="line">lrwxrwxrwx. 1 root root 16 Jan 30 10:16 net_cls -&gt; net_cls,net_prio</span><br><span class="line">dr-xr-xr-x. 3 root root  0 Jan 30 10:16 net_cls,net_prio</span><br><span class="line">lrwxrwxrwx. 1 root root 16 Jan 30 10:16 net_prio -&gt; net_cls,net_prio</span><br><span class="line">dr-xr-xr-x. 3 root root  0 Jan 30 10:16 perf_event</span><br><span class="line">dr-xr-xr-x. 6 root root  0 Jan 30 10:16 pids</span><br><span class="line">dr-xr-xr-x. 2 root root  0 Jan 30 10:16 rdma</span><br><span class="line">dr-xr-xr-x. 6 root root  0 Jan 30 10:16 systemd</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo mount -t cgroup</span></span><br><span class="line">cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)</span><br><span class="line">cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,perf_event)</span><br><span class="line">cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,memory)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /sys/fs/cgroup/cpu</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir <span class="built_in">test</span> &amp;&amp; <span class="built_in">cd</span> <span class="built_in">test</span> &amp;&amp; ls</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 发现新建的<span class="built_in">test</span>目录里已经有自动生成的一些文件了，我们可以写这些文件来限制某一个进程的资源上限</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 我们先来运行一个进程，死循环，把CPU占满，可以通过top查看</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="keyword">while</span> : ; <span class="keyword">do</span> : ; <span class="keyword">done</span></span> </span><br><span class="line">&amp;[1] 226</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 然后我们可以通过写cgroup fs来限制这个进程的cpu资源上限</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> tasks里面是要限制的进程的PID</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> 226 &gt; tasks</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认-1，表示没有上限</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us</span> </span><br><span class="line">-1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 100ms</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /sys/fs/cgroup/cpu/container/cpu.cfs_period_us</span> </span><br><span class="line">100000</span><br><span class="line"><span class="meta">#</span><span class="bash">20ms</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> 20000 &gt; /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 说明：每100ms里，被这个cgroup控制的进程，也就是226，只能使用20ms的时间。尽管是个死循环，加了限制之后cpu占用率不会高于20%，牛！</span></span><br></pre></td></tr></table></figure>
<h4 id="cgroup的一些限制"><a href="#cgroup的一些限制" class="headerlink" title="cgroup的一些限制"></a>cgroup的一些限制</h4><p>上来我们用到的<code>top</code>命令，主要的数据来源都是<code>/proc</code>目录下。这个/proc目录记录当前内核运行状态的数据，它并不知道cgroup限制了一个容器什么。</p>
<p>所以在容器里运行top时，它也会显示host的cpu，内存信息。</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机网络概述</title>
    <url>/posts/7c4ca347.html</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="1-速率相关性能指标"><a href="#1-速率相关性能指标" class="headerlink" title="1. 速率相关性能指标"></a>1. 速率相关性能指标</h3><h4 id="速率"><a href="#速率" class="headerlink" title="速率"></a>速率</h4><p>在数字信道上传输数据位的速度。单位: b/s,Kb/s,Mb/s,Tb/s，<br>如果用字节表示，则是B/s,KB/s,MB/s,TB/s（1Byte=8Bit）</p>
<h4 id="带宽"><a href="#带宽" class="headerlink" title="带宽"></a>带宽</h4><p>单位同速率，指理想条件下的最高速率。</p>
<h4 id="吞吐量"><a href="#吞吐量" class="headerlink" title="吞吐量"></a>吞吐量</h4><p>单位时间内通过某个网络的数据总量。</p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>比如说1000M宽带的路由器连着三部手机，每部手机都是10mb/s看片，那么速率就是10mb/s，带宽是宽带的1000m，路由器吞吐量是30mb/s。</p>
<img src="/posts/7c4ca347/image-20210208111213048.png" alt="image-20210208111213048" style="zoom:50%;">



<h3 id="2-时延相关性能指标"><a href="#2-时延相关性能指标" class="headerlink" title="2. 时延相关性能指标"></a>2. 时延相关性能指标</h3><h4 id="时延"><a href="#时延" class="headerlink" title="时延"></a>时延</h4><table>
<thead>
<tr>
<th>名称</th>
<th>描述</th>
<th>计算公式</th>
</tr>
</thead>
<tbody><tr>
<td>发送时延</td>
<td>数据从主机到信道上所用的时间</td>
<td>发送的数据长度/发送速率</td>
</tr>
<tr>
<td>传播时延</td>
<td>数据在信道上传播所花费的时间</td>
<td>信道长度/电磁波在信道上传播的速率</td>
</tr>
<tr>
<td>排队时延</td>
<td>数据在路由器前等待前面数据处理的时间</td>
<td>无计算方式</td>
</tr>
<tr>
<td>处理时延</td>
<td>数据在路由器中处理需求的时间</td>
<td>无计算方式</td>
</tr>
</tbody></table>
<h4 id="时延带宽积"><a href="#时延带宽积" class="headerlink" title="时延带宽积"></a>时延带宽积</h4><p>传播时延*带宽</p>
<p>表示整个链路上总共有多少bit的数据。</p>
<h4 id="往返时间RTT"><a href="#往返时间RTT" class="headerlink" title="往返时间RTT"></a>往返时间RTT</h4><p>从发送方发送数据开始，到接受方确认接受到数据为止花费的时间</p>
<p>= 传播时延*2 + 处理时延</p>
<h3 id="3-利用率"><a href="#3-利用率" class="headerlink" title="3. 利用率"></a>3. 利用率</h3><h4 id="信道利用率"><a href="#信道利用率" class="headerlink" title="信道利用率"></a>信道利用率</h4><p>=  有数据通过的时间/（有+无数据通过的时间）</p>
<h4 id="网络利用率"><a href="#网络利用率" class="headerlink" title="网络利用率"></a>网络利用率</h4><p>= 所有信道利用率加权求平均值</p>
<h4 id="时延和利用率的关系"><a href="#时延和利用率的关系" class="headerlink" title="时延和利用率的关系"></a>时延和利用率的关系</h4><img src="/posts/7c4ca347/image-20210208111943694.png" alt="image-20210208111943694">



<h3 id="4-网络的分层结构"><a href="#4-网络的分层结构" class="headerlink" title="4. 网络的分层结构"></a>4. 网络的分层结构</h3><p><img src="/posts/7c4ca347/image-20210208112313439.png" alt="image-20210208112313439"></p>
<p>结合TCP/IP参考模型和OSI参考模型，现在广泛使用的是五层参考模型，如下图：</p>
<p><img src="/posts/7c4ca347/image-20210208112446492.png" alt="image-20210208112446492"></p>
<p>下面将按照这五层一次介绍。</p>
<h2 id="物理层"><a href="#物理层" class="headerlink" title="物理层"></a>物理层</h2><p>主要负责传输数据比特流。</p>
<h4 id="码元"><a href="#码元" class="headerlink" title="码元"></a>码元</h4><p>一个码元就是一个脉冲信号，如果这个脉冲信号是矩形脉冲，只有高低两个电平，则只能表示0或者1，也就是一个比特位。如果这个脉冲信号有8种变化（可以是电平高低，相位变化等），则可以表示3位比特位（000-111）一共八种。</p>
<p><strong>结论</strong>：一个码元就是一个脉冲，一个脉冲有N中变化，则可以表示log<sub>2</sub>N位bit。</p>
<h4 id="波特（Baud）"><a href="#波特（Baud）" class="headerlink" title="波特（Baud）"></a>波特（Baud）</h4><p>一秒可以传输多少个码元。</p>
<h4 id="速率-1"><a href="#速率-1" class="headerlink" title="速率"></a>速率</h4><p>分为<strong>码元传输速率</strong>和<strong>信息传输速率</strong></p>
<p>信息传输速率就是b/s，就是我们平常说的<strong>网速</strong></p>
<p>码元可以理解为几个比特的<strong>集合</strong>，所以信息传输速率（网速）=码元传输速率x码元所带信息量（多少比特）</p>
<h4 id="三种通讯模式"><a href="#三种通讯模式" class="headerlink" title="三种通讯模式"></a>三种通讯模式</h4><p>单工：只能一个发一个收，需要1条信道</p>
<p>半双工：都可以发或者收，但同一时间只能进行一个，需要2条信道</p>
<p>全双工：可以同时收和发，需要2条信道</p>
<h4 id="两种数据传输模式"><a href="#两种数据传输模式" class="headerlink" title="两种数据传输模式"></a>两种数据传输模式</h4><p>串行：速度慢，省钱，适合远距离 </p>
<p>并行：速度快，花钱，适合近距离</p>
<p><img src="/posts/7c4ca347/image-20210208115132757.png" alt="image-20210208115132757"></p>
<h4 id="工作在物理层的设备"><a href="#工作在物理层的设备" class="headerlink" title="工作在物理层的设备"></a>工作在物理层的设备</h4><h5 id="中继器"><a href="#中继器" class="headerlink" title="中继器"></a>中继器</h5><p>当数据离开源在网络上传送时，它是转换为能够沿着网络介质传输的电脉冲或光脉冲的——这些脉冲称为信号（signal）。当信号沿着网络介质进行传送时， 随着经过的线缆越来越长，信号就会变得越来越弱，越来越差。中继器的目的是在<strong>比特级别</strong>对网络信号进行<strong>再生和重定</strong>时，从而使得它们能够在网络上传输更长的距离。</p>
<p>作用：对信号进行放大和整形，仅适用于以太网。</p>
<img src="/posts/7c4ca347/image-20210208161145175.png" alt="image-20210208161145175" style="zoom:150%;">

<h5 id="集线器（HUB）"><a href="#集线器（HUB）" class="headerlink" title="集线器（HUB）"></a>集线器（HUB）</h5><p>也有放大和整形的功能，可以看作是一个多端口的中继器。</p>
<p>HUB是网络中各个设备的通用连接点，它通常用于连接LAN的分段。HUB含有多个端口。每一个分组到达某个端口时，都会被复制到其他所有端口（广播），以便所有的LAN分段都能看见所有的分组。集线器并不认识信号、地址或数据中任何信息模式。</p>
<h6 id="集线器产生的思路："><a href="#集线器产生的思路：" class="headerlink" title="集线器产生的思路："></a>集线器产生的思路：</h6><p>一开始，电脑之前就通过网线连接，当电脑数量增多时，就会很乱，网络变得异常复杂。<img src="/posts/7c4ca347/image-20210208161725908.png" alt="image-20210208161725908" style="zoom:50%;"></p>
<p>所以我们在这些电脑的中间加一个设备——集线器。</p>
<img src="/posts/7c4ca347/image-20210208162211908.png" alt="image-20210208162211908" style="zoom:67%;">



<p>每个电脑插在集线器的一个端口上。这样每台电脑发送数据都要通过集线器。比如A要给E发消息，那么A发出一个包，这个包到达hub之后，会被hub发往各个端口，BCDE都能收到，只有E收到这个包，发现包头的MAC标识的时自己，于是接收这个包，其他的BCD都丢弃这个包。</p>
]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title>网络虚拟化简介</title>
    <url>/posts/e41fb1bd.html</url>
    <content><![CDATA[<img src="/posts/e41fb1bd/image-20210201221242667.png" alt="image-20210201221242667" style="zoom:70%;">

]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
      <tags>
        <tag>network</tag>
        <tag>virtualization</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx简介</title>
    <url>/posts/cc1b6e67.html</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul>
<li>反向代理</li>
<li>负载均衡</li>
<li>动静分离</li>
</ul>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo yum install nginx</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo systemctl start nginx.service</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 访问成功</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> curl localhost:80</span></span><br></pre></td></tr></table></figure>
<p><strong>测试安装成功</strong></p>
<p>因为我的测试环境是vagrant vm，所以需要做一个端口映射，这样我们可以在host上访问。</p>
<img src="/posts/cc1b6e67/image-20210201230936553.png" alt="image-20210201230936553" style="zoom:50%;">

<p>成功！</p>
<img src="/posts/cc1b6e67/image-20210201231043622.png" alt="image-20210201231043622" style="zoom:50%;">

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nginx -h</span></span><br><span class="line">nginx version: nginx/1.14.1</span><br><span class="line">Usage: nginx [-?hvVtTq] [-s signal] [-c filename] [-p prefix] [-g directives]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">...</span><br><span class="line">  -s signal     : send signal to a master process: stop, quit, reopen, reload</span><br><span class="line">  -p prefix     : set prefix path (default: /usr/share/nginx/)</span><br><span class="line">  -c filename   : set configuration file (default: /etc/nginx/nginx.conf)</span><br><span class="line">  -g directives : set global directives out of configuration file</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> nginx -s stop 停止</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> nginx -s quit 安全退出</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> nginx -s reload 重新加载配置文件</span></span><br></pre></td></tr></table></figure>
<h3 id="nginx配置文件"><a href="#nginx配置文件" class="headerlink" title="nginx配置文件"></a>nginx配置文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim /etc/nginx/nginx.conf</span></span><br></pre></td></tr></table></figure>
<p>可以看到nginx server是监听在80端口的，我们可以修改端口，然后用<code>nginx -s reload</code>来生效。</p>
<img src="/posts/cc1b6e67/image-20210201231634344.png" alt="image-20210201231634344" style="zoom:50%;">



<h3 id="详解nginx-conf"><a href="#详解nginx-conf" class="headerlink" title="详解nginx.conf"></a>详解nginx.conf</h3><p> 三部分</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">user  www;# 工作进程的属主</span><br><span class="line"> worker_processes  4;# 工作进程数，一般与 CPU 核数等同</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">error_log  logs/error.log;</span></span><br><span class="line"><span class="meta"> #</span><span class="bash">error_log  logs/error.log  notice;</span></span><br><span class="line"><span class="meta"> #</span><span class="bash">error_log  logs/error.log  info;</span></span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">pid        logs/nginx.pid;</span></span><br><span class="line"> </span><br><span class="line"> events &#123;</span><br><span class="line">    use epoll;#Linux 下性能最好的 event 模式</span><br><span class="line">    worker_connections  2048;# 每个工作进程允许最大的同时连接数</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> http &#123;</span><br><span class="line">		upstream xxx&#123;</span><br><span class="line">				server1:port1</span><br><span class="line">				server2:port3</span><br><span class="line">		&#125; </span><br><span class="line">		</span><br><span class="line">		server &#123;</span><br><span class="line">				location / &#123;</span><br><span class="line">						proxy_pass xxx</span><br><span class="line">				&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p><a href="https://www.cnblogs.com/fengzhongzhuzu/p/8848115.html">https://www.cnblogs.com/fengzhongzhuzu/p/8848115.html</a></p>
<h3 id="Nginx的负载均衡调度算法"><a href="#Nginx的负载均衡调度算法" class="headerlink" title="Nginx的负载均衡调度算法"></a>Nginx的负载均衡调度算法</h3><h4 id="weight轮询-默认，常用"><a href="#weight轮询-默认，常用" class="headerlink" title="weight轮询(默认，常用)"></a>weight轮询(默认，常用)</h4><p>收到的请求<strong>按照权重分配</strong>到不同的后端服务器，即使在使用过程中，某一台后端服务器宕机，Nginx会自动将该服务器剔除出队列，请求受理情况不会受到任何影响。这种方式下，可以给不同的后端服务器设置一个权重值(weight)，用于调整不同的服务器上请求的分配率；权重数据越大，被分配到请求的几率越大；该权重值，主要是针对实际工作环境中不同的后端服务器硬件配置进行调整的。</p>
<h4 id="ip-hash（常用）"><a href="#ip-hash（常用）" class="headerlink" title="ip_hash（常用）"></a>ip_hash（常用）</h4><p>每个请求按照发起客户端的ip的hash结果进行匹配，这样的算法下一个固定ip地址的客户端总会访问到同一个后端服务器，这也在一定程度上解决了集群部署环境下<strong>session共享的</strong>问题。</p>
<h4 id="fair智能调整调度算法"><a href="#fair智能调整调度算法" class="headerlink" title="fair智能调整调度算法"></a>fair智能调整调度算法</h4><p>动态的根据后端服务器的请求处理到响应的时间进行均衡分配，响应时间短处理效率高的服务器分配到请求的概率高，响应时间长处理效率低的服务器分配到的请求少；结合了前两者的优点的一种调度算法。但是需要注意的是Nginx默认不支持fair算法，如果要使用这种调度算法，请安装upstream_fair模块。</p>
<h4 id="url-hash"><a href="#url-hash" class="headerlink" title="url_hash"></a>url_hash</h4><p>按照访问的url的hash结果分配请求，每个请求的url会指向后端固定的某个服务器，可以在Nginx作为静态服务器的情况下提高缓存效率。同样要注意Nginx默认不支持这种调度算法，要使用的话需要安装Nginx的hash软件包。</p>
]]></content>
  </entry>
  <entry>
    <title>containerPort, targetPort, port, nodePort等的区别</title>
    <url>/posts/3836ec52.html</url>
    <content><![CDATA[<p>本文会通过container中的应用的几种访问方式来解释containerPort, targetPort, port, nodePort的关系和区别。</p>
<h2 id="containerPort"><a href="#containerPort" class="headerlink" title="containerPort"></a>containerPort</h2><p>容器暴露的端口，在container spec里声明。</p>
<img src="/posts/3836ec52/image-20210308182307042.png" alt="image-20210308182307042" style="zoom:50%;">

<p>可以通过podIP:containerPort来访问容器内应用。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> k get pod -owide</span></span><br><span class="line">NAME    READY   STATUS    RESTARTS   AGE    IP                  NODE  </span><br><span class="line">pod1    2/2     Running   0          133m   192.168.227.89   otcloud-node1   </span><br><span class="line">pod2    2/2     Running   0          47m    192.168.227.97   otcloud-node1   </span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">可以访问容器内应用</span></span><br><span class="line">curl 192.168.227.97:9080</span><br></pre></td></tr></table></figure>
<p><strong>Notes：</strong></p>
<p>如果一个pod里声明了多个容器，那么每个容器暴露的端口必须不同，否则容器启动会失败。（Address already in use）</p>
<p><strong>更进一步</strong>，因为一个pod里的所有容器都共享网络namespace，所以它们共享IP，和端口。</p>
<h2 id="targetPort"><a href="#targetPort" class="headerlink" title="targetPort"></a>targetPort</h2><p>是<strong>Service</strong>资源里的一个概念，在service的声明中使用。</p>
<p>targetPort是Pod上的端口，从port/nodePort上来的数据，经过kube-proxy流入到后端pod的targetPort上，最后进入容器。</p>
<p>如果不指明targetPort，默认targetPort和service暴露的port一致。</p>
<p>下面是一个service的声明：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">productpage</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">productpage</span></span><br><span class="line">    <span class="attr">service:</span> <span class="string">productpage</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">9999</span>  <span class="comment"># clusterip:port </span></span><br><span class="line">    <span class="comment"># targetPort: 9080 # 如果不写，默认是9999</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">productpage</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> k describe svc productpage</span><br><span class="line">Name:              productpage</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            app=productpage</span><br><span class="line">                   service=productpage</span><br><span class="line">Annotations:       &lt;none&gt;</span><br><span class="line">Selector:          app=productpage</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP Families:       &lt;none&gt;</span><br><span class="line">IP:                10.97.239.58</span><br><span class="line">IPs:               10.97.239.58</span><br><span class="line">Port:              http  9999/TCP  </span><br><span class="line">TargetPort:        9999/TCP  # 9999 和上面container暴露的接口不一致，访问一定会失败</span><br><span class="line">Endpoints:         192.168.227.97:9999</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>解决方法：</strong></p>
<ol>
<li>显式地声明targetPort: 9080</li>
<li>使用9080作为port，这样targetport也是9080，就可以和container port连接起来。</li>
</ol>
<p>这里我们把targetPort显式的写成9080，会得到：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> k describe svc productpage</span><br><span class="line">Name:              productpage</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            app=productpage</span><br><span class="line">                   service=productpage</span><br><span class="line">Annotations:       &lt;none&gt;</span><br><span class="line">Selector:          app=productpage</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP Families:       &lt;none&gt;</span><br><span class="line">IP:                10.97.239.58 # cluster IP！</span><br><span class="line">IPs:               10.97.239.58</span><br><span class="line">Port:              http  9999/TCP # 注意！</span><br><span class="line">TargetPort:        9080/TCP  # 注意！</span><br><span class="line">Endpoints:         192.168.227.97:9080 # 注意！</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></figure>
<p>这个service的细节要好好研究一下。</p>
<p><strong>IP：</strong>Service的cluster IP。</p>
<p><strong>Port：</strong>Service的cluster IP暴露的接口。</p>
<p><strong>TargetPort：</strong>容器暴露的接口。</p>
<p><strong>Endpoint：</strong> podIP:targetPort，注意这里是targetPort，不是containerPort。</p>
<p>又有访问容器内应用的新方式：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> clusterIP:port</span></span><br><span class="line">curl 10.97.239.58:9999</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> by endpoint, 跟containerPort里讲到的实际是一样的</span></span><br><span class="line">curl 192.168.227.97:9080</span><br></pre></td></tr></table></figure>


<h2 id="Port"><a href="#Port" class="headerlink" title="Port"></a>Port</h2><p>上面讲的差不多了，也是<strong>service</strong>里的概念，是指在clusterIP上暴露的端口，是一个虚拟的东西。</p>
<p>clusterIP：虚拟的IP，在集群内可以相互访问。它只是存在service的规则当中。 <strong>注意，它不是podIP。</strong></p>
<h2 id="nodePort"><a href="#nodePort" class="headerlink" title="nodePort"></a>nodePort</h2><p>nodePort 提供了<strong>集群外</strong>部客户端访问 Service 的一种方式，nodePort 提供了集群外部客户端访问 Service 的端口，通过 <code>nodeIP:nodePort</code> 提供了外部流量访问k8s集群中service的入口。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> k get svc -owide</span></span><br><span class="line">NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE    SELECTOR</span><br><span class="line">productpage   NodePort    10.97.239.58   &lt;none&gt;       9999:30066/TCP  25m app=productpage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> k describe svc productpage</span></span><br><span class="line">Name:                     productpage</span><br><span class="line">Namespace:                default</span><br><span class="line">Labels:                   app=productpage</span><br><span class="line">                          service=productpage</span><br><span class="line">Annotations:              &lt;none&gt;</span><br><span class="line">Selector:                 app=productpage</span><br><span class="line">Type:                     NodePort</span><br><span class="line">IP Families:              &lt;none&gt;</span><br><span class="line">IP:                       10.97.239.58</span><br><span class="line">IPs:                      10.97.239.58</span><br><span class="line">Port:                     http  9999/TCP</span><br><span class="line">TargetPort:               9080/TCP</span><br><span class="line">NodePort:                 http  30066/TCP  # 在host node上暴露的端口</span><br><span class="line">Endpoints:                192.168.227.97:9080</span><br><span class="line">Session Affinity:         None</span><br><span class="line">External Traffic Policy:  Cluster</span><br><span class="line">Events:                   &lt;none&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>又一种新的访问方式：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">hostIP:nodeport</span></span><br><span class="line">curl localhost:30066</span><br></pre></td></tr></table></figure>


<p>总结：</p>
<p><img src="/posts/3836ec52/image-20210308190947963.png" alt="image-20210308190947963"></p>
<p>三种访问方式以及范围：</p>
<table>
<thead>
<tr>
<th>方式</th>
<th>范围</th>
</tr>
</thead>
<tbody><tr>
<td>podIP:targetPort</td>
<td>和pod在同一个网络下</td>
</tr>
<tr>
<td>clusterIP:port</td>
<td>在集群中</td>
</tr>
<tr>
<td>hostIP:nodeport</td>
<td>主机之间也可以访问</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title>进程和线程</title>
    <url>/posts/6ac22eb.html</url>
    <content><![CDATA[<img src="/posts/6ac22eb/image-20210225180738908.png" alt="image-20210225180738908" style="zoom:67%;">

<blockquote>
<p>参考：<a href="https://cloud.tencent.com/developer/article/1688297">https://cloud.tencent.com/developer/article/1688297</a></p>
</blockquote>
<h2 id="简单的理解"><a href="#简单的理解" class="headerlink" title="简单的理解"></a>简单的理解</h2><p>对于操作系统来说，一个任务就是一个进程（<strong>Process</strong>），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。</p>
<p>有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（<strong>Thread</strong>）。</p>
<h2 id="并行和并发"><a href="#并行和并发" class="headerlink" title="并行和并发"></a>并行和并发</h2><p>并行是真正的同时运行，需要多核。</p>
<p>并发只是看起来同时运行，最常见的就是<strong>时间片轮转调度算法</strong>。</p>
<h2 id="进程和线程的区别"><a href="#进程和线程的区别" class="headerlink" title="进程和线程的区别"></a>进程和线程的区别</h2><p><strong>共享同一地址空间</strong>（也就是同样的<strong>动态内存，映射文件，目标代码等等</strong>），<strong>打开的文件队列和其他内核资源</strong>。</p>
<h5 id><a href="#" class="headerlink" title></a></h5><table>
<thead>
<tr>
<th align="center">进程</th>
<th align="center">线程</th>
</tr>
</thead>
<tbody><tr>
<td align="center">有独立的地址空间</td>
<td align="center">有自己的栈和局部变量，但没有单独的地址空间。<br>同一进程中的多个线程共享代码段（代码和常量），数据段（全局变量和静态变量），扩展段（堆存储）。<br>但是每个线程拥有自己的栈段，栈段又叫运行时段，用来存放所有局部变量和临时变量。）</td>
</tr>
<tr>
<td align="center">在保护模式下不会对其它进程产生影响</td>
<td align="center">一个线程死掉就等于整个进程死掉</td>
</tr>
<tr>
<td align="center">拥有独立的内存单元</td>
<td align="center">共享进程的内存</td>
</tr>
<tr>
<td align="center">管道、系统IPC（包括<a href="https://cloud.tencent.com/product/cmq?from=10680">消息队列</a>、信号量、信号、共享内存等）、以及套接字socket</td>
<td align="center">可以直接读写进程数据段和全局变量（互斥锁，信号量）</td>
</tr>
<tr>
<td align="center">上下文切换慢</td>
<td align="center">进程上下文切换快</td>
</tr>
<tr>
<td align="center"><strong>进程是操作系统分配资源的单位</strong></td>
<td align="center"><strong>是CPU调度和分派执行的基本单位</strong></td>
</tr>
<tr>
<td align="center">进程编程调试简单可靠性高，但是创建销毁开销大</td>
<td align="center">线程正相反，开销小，切换速度快，但是编程调试相对复杂</td>
</tr>
</tbody></table>
<p>创建线程的代价比创建进程的代价小很多，如下图：</p>
<p><img src="/posts/6ac22eb/image-20210225181636565.png" alt="image-20210225181636565"></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title>QAT在Ubuntu18.04上的安装和虚拟化</title>
    <url>/posts/461bb783.html</url>
    <content><![CDATA[<p>本文将介绍QAT c62xx设备在Ubuntu 18.04上的安装部署和虚拟化。</p>
<blockquote>
<p>参考网站：<a href="https://01.org/intel-quickassist-technology">https://01.org/intel-quickassist-technology</a></p>
</blockquote>
<h3 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux tme-prc002 5.3.0-53-generic #47~18.04.1-Ubuntu SMP Thu May 7 13:10:50 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure>
<h3 id="Updating-grub-Configuration-File"><a href="#Updating-grub-Configuration-File" class="headerlink" title="Updating grub Configuration File"></a>Updating grub Configuration File</h3><p>To enable IOMMU and SRIOV</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/default/grub</span><br></pre></td></tr></table></figure>
<p>Modify <code>GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet splash&quot;</code> to <code>&quot;quiet splash intel_iommu=on&quot;</code></p>
<p>Update grub and reboot:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">update-grub</span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure>
<h3 id="Building-and-Installing-Software"><a href="#Building-and-Installing-Software" class="headerlink" title="Building and Installing Software"></a>Building and Installing Software</h3><h4 id="Create-a-working-directory-for-the-software-This-directory-can-be-user-defined-but"><a href="#Create-a-working-directory-for-the-software-This-directory-can-be-user-defined-but" class="headerlink" title="Create a working directory for the software. This directory can be user defined, but"></a>Create a working directory for the software. This directory can be user defined, but</h4><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="variable">$</span> <span class="built_in">cd</span> <span class="variable">$HOME</span> </span><br><span class="line"><span class="variable">$</span> mkdir /QAT</span><br><span class="line"><span class="variable">$</span> <span class="built_in">cd</span> /QAT</span><br></pre></td></tr></table></figure>
<h4 id="Download-the-software"><a href="#Download-the-software" class="headerlink" title="Download the software."></a>Download the software.</h4><figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">curl -O https:<span class="regexp">//</span><span class="number">01</span>.org<span class="regexp">/sites/</span>default<span class="regexp">/files/</span>downloads<span class="regexp">/intelr-quickassist-technology/</span>qat1.<span class="number">7</span>.l.<span class="number">4.3</span>.<span class="number">0</span>-<span class="number">00033</span>.tar.gz</span><br></pre></td></tr></table></figure>
<h4 id="Unpack-the-software"><a href="#Unpack-the-software" class="headerlink" title="Unpack the software."></a>Unpack the software.</h4><figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line"><span class="variable">$ </span>tar -zxof &lt;QAT tarball name&gt;  </span><br><span class="line"><span class="variable">$ </span>chmod -R o-rwx *</span><br></pre></td></tr></table></figure>
<h4 id="Install-dependencies-if-needed"><a href="#Install-dependencies-if-needed" class="headerlink" title="Install dependencies if needed."></a>Install dependencies if needed.</h4><p>For CentOS:</p>
<figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line"><span class="variable">$ </span>yum -y groupinstall <span class="string">&quot;Development Tools&quot;</span></span><br><span class="line"><span class="variable">$ </span>yum -y install pciutils</span><br><span class="line"><span class="variable">$ </span>yum -y install libudev-devel</span><br><span class="line"><span class="variable">$ </span>yum -y install kernel-devel-<span class="variable">$(</span>uname -r)</span><br><span class="line"><span class="variable">$ </span>yum -y install gcc</span><br><span class="line"><span class="variable">$ </span>yum -y install openssl-devel</span><br></pre></td></tr></table></figure>
<p>For Ubuntu:</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">$ apt-<span class="builtin-name">get</span> update</span><br><span class="line">$ apt-<span class="builtin-name">get</span> install pciutils-dev</span><br><span class="line">$ apt-<span class="builtin-name">get</span> install g++</span><br><span class="line">$ apt-<span class="builtin-name">get</span> install pkg-config</span><br><span class="line">$ apt-<span class="builtin-name">get</span> install libssl-dev</span><br></pre></td></tr></table></figure>
<h4 id="configure-make-and-make-install-the-software-packages"><a href="#configure-make-and-make-install-the-software-packages" class="headerlink" title="configure, make and make install the software packages."></a>configure, make and make install the software packages.</h4><figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line"><span class="comment"># suggest to clear up the env if there is a previous or modified version installed</span></span><br><span class="line"><span class="variable">$ </span>make uninstall</span><br><span class="line"><span class="variable">$ </span>./configure --enable-icp-sriov=host  <span class="comment"># It fails if missing dependency package.</span></span><br><span class="line"><span class="variable">$ </span>make</span><br><span class="line"><span class="variable">$ </span>make install  </span><br><span class="line"><span class="variable">$ </span>service qat_service start  </span><br><span class="line"><span class="variable">$ </span>service qat_service_vfs start</span><br></pre></td></tr></table></figure>
<p>NOTE: in Guest OS, enable the SR-IOV build on the host by using:</p>
<figure class="highlight jboss-cli"><table><tr><td class="code"><pre><span class="line">$ <span class="string">./configure</span> <span class="params">--enable-icp-sriov=guest</span></span><br></pre></td></tr></table></figure>
<h4 id="Verifying-SR-IOV-On-The-Host"><a href="#Verifying-SR-IOV-On-The-Host" class="headerlink" title="Verifying SR-IOV On The Host"></a>Verifying SR-IOV On The Host</h4><figure class="highlight llvm"><table><tr><td class="code"><pre><span class="line">$ lspci | grep <span class="number">37</span><span class="keyword">c</span><span class="number">9</span>  </span><br><span class="line"># <span class="keyword">or</span> </span><br><span class="line">$ lspci -d <span class="number">8086</span>:<span class="number">37</span><span class="keyword">c</span><span class="number">9</span></span><br><span class="line">$ lspci -d:<span class="number">37</span><span class="keyword">c</span><span class="number">9</span></span><br><span class="line">...</span><br><span class="line"><span class="number">01</span>:<span class="number">01.0</span> Co-processor: Intel Corporation Device <span class="number">37</span><span class="keyword">c</span><span class="number">9</span></span><br><span class="line"><span class="number">01</span>:<span class="number">01.1</span> Co-processor: Intel Corporation Device <span class="number">37</span><span class="keyword">c</span><span class="number">9</span></span><br><span class="line"><span class="number">01</span>:<span class="number">01.2</span> Co-processor: Intel Corporation Device <span class="number">37</span><span class="keyword">c</span><span class="number">9</span></span><br><span class="line">...</span><br><span class="line"><span class="number">01</span>:<span class="number">01.7</span> Co-processor: Intel Corporation Device <span class="number">37</span><span class="keyword">c</span><span class="number">9</span></span><br><span class="line"><span class="number">01</span>:<span class="number">02.0</span> Co-processor: Intel Corporation Device <span class="number">37</span><span class="keyword">c</span><span class="number">9</span></span><br><span class="line"><span class="number">01</span>:<span class="number">02.1</span> Co-processor: Intel Corporation Device <span class="number">37</span><span class="keyword">c</span><span class="number">9</span></span><br><span class="line">...</span><br><span class="line"><span class="number">01</span>:<span class="number">02.6</span> Co-processor: Intel Corporation Device <span class="number">37</span><span class="keyword">c</span><span class="number">9</span></span><br><span class="line"><span class="number">01</span>:<span class="number">02.7</span> Co-processor: Intel Corporation Device <span class="number">37</span><span class="keyword">c</span><span class="number">9</span></span><br></pre></td></tr></table></figure>
<p>Or using <code>adf_ctl</code></p>
<figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line"><span class="variable">$ </span>adf_ctl status</span><br></pre></td></tr></table></figure>
<p>This utility can bring up/down device by ourselves.</p>
<h4 id="Detailed-SR-IOV-enabling-please-refer-to-the-guide"><a href="#Detailed-SR-IOV-enabling-please-refer-to-the-guide" class="headerlink" title="Detailed SR-IOV enabling please refer to the guide."></a>Detailed SR-IOV enabling please refer to the guide.</h4><p>Please refer to:<br><a href="https://01.org/sites/default/files/downloads/330689qatvirtualizationappnoterev008us.pdf">https://01.org/sites/default/files/downloads/330689qatvirtualizationappnoterev008us.pdf</a></p>
]]></content>
      <categories>
        <category>deploy</category>
      </categories>
      <tags>
        <tag>qat</tag>
      </tags>
  </entry>
  <entry>
    <title>代理服务器基本介绍</title>
    <url>/posts/65bbead6.html</url>
    <content><![CDATA[<h2 id="为什么需要proxy-server"><a href="#为什么需要proxy-server" class="headerlink" title="为什么需要proxy server?"></a>为什么需要proxy server?</h2><ol>
<li><p>监控，内容过滤</p>
<ul>
<li>公司的proxy server可以清楚的知道，你的流量要访问什么网站。proxy server可以禁掉一些网址，或者redirect到一些提示页面。</li>
<li>公司的proxy server可以监控你每天都在访问什么网站，花了多长时间。</li>
</ul>
</li>
<li><p>节约带宽，节省成本</p>
<ul>
<li>在proxy server上会有一些cache，里面存放了最近访问的website的数据，如果有1000个request通过同一个proxy server访问website A，这时如果proxy server上有website A的最新数据在cache里，就可以直接返回给这1000个client。如果proxy server上没有，也只需要发一个website A的请求，然后把返回数据放在cache里。</li>
</ul>
</li>
<li><p>提高性能</p>
<ul>
<li>通过代理服务器的缓存（比如 CDN）和负载均衡（比如 nginx lb）功能，服务器端可以加速请求的访问，在更快的时间内返回结果）</li>
</ul>
</li>
<li><p>安全和隐私</p>
<ul>
<li><p>有一些proxy server可以隐藏掉client的IP等个人信息，这样通过proxy访问网站，web server并不能知道是谁在真正的访问它。</p>
</li>
<li><p>公司可以在内网和外网之间通过代理进行转发，这样不仅对外隐藏了实现的细节，而且可以在代理层对爬虫、病毒性请求进行过滤，保护内部服务。</p>
</li>
<li><p>还可以加密数据</p>
</li>
</ul>
</li>
</ol>
<blockquote>
<p>风险：</p>
<p>因为所有client的请求都要通过proxy server访问internet，所以有数据泄露的风险。也有一些技术可以实现数据加密，规避这种risk，后面会讲到。</p>
</blockquote>
<h2 id="代理服务器的分类："><a href="#代理服务器的分类：" class="headerlink" title="代理服务器的分类："></a>代理服务器的分类：</h2><ol>
<li><h4 id="按照匿名功能分类（是否具有隐藏IP的功能）"><a href="#按照匿名功能分类（是否具有隐藏IP的功能）" class="headerlink" title="按照匿名功能分类（是否具有隐藏IP的功能）"></a>按照匿名功能分类（是否具有隐藏IP的功能）</h4><ul>
<li><p>非匿名（和透明代理的区别？？？）</p>
</li>
<li><p>匿名：使用此种代理时，虽然被访问的网站不能知道你的 IP 地址，但仍然可 以知道你在使用代理，有些侦测 IP 的网页也仍然可以查到你的 IP。</p>
</li>
<li><p>高度匿名：使用此种代理时，被访问的网站不知道你的 IP 地址，也不知道你在使用代理进行访问。此种代理的隐藏 IP 地址的功能最强。</p>
</li>
<li><p>透明代理（简单代理）：透明代理的意思是客户端根本不需要知道有代理服务器的存在，它改编你的 request fields（报文），并会传送真实 IP。web server会直接获得client的IP，并且知道client是通过proxy访问的它。</p>
</li>
</ul>
</li>
<li><h4 id="按代理服务器的用途分类"><a href="#按代理服务器的用途分类" class="headerlink" title="按代理服务器的用途分类"></a>按代理服务器的用途分类</h4><ul>
<li>HTTP代理</li>
<li>SSL代理</li>
<li>HTTP CONNECT代理</li>
<li>FTP代理</li>
<li>POP3代理</li>
<li>Telnet代理</li>
<li>Socks代理</li>
<li>等等等等</li>
</ul>
</li>
<li><h4 id="按照相对于client-server的角色分类"><a href="#按照相对于client-server的角色分类" class="headerlink" title="按照相对于client-server的角色分类"></a>按照相对于client-server的角色分类</h4><h5 id="正向代理"><a href="#正向代理" class="headerlink" title="正向代理"></a>正向代理</h5><p>代理对客户端不是透明的，客户端需要知道代理的地址并且手动配置。配置了代理，浏览器在发送请求的时候会对报文做特殊的修改。</p>
<h5 id="反向代理"><a href="#反向代理" class="headerlink" title="反向代理"></a>反向代理</h5><p>说客户端一般不知道代理的存在，认为自己是直接和服务器通信。我们大部分访问的网站就是反向代理服务器，反向代理服务器会转发到真正的服务器，一般在反向代理这一层实现<strong>负载均衡</strong>和<strong>高可用</strong>的功能。而且这里也可以看到，客户端是不会知道真正服务器端的 ip 地址和端口的，这在一定程度上起到了安全保护的作用。</p>
<img src="/posts/65bbead6/image-20210129182705751.png" alt="image-20210129182705751" style="zoom:60%;"></li>
</ol>
<h2 id="代理具体做什么？"><a href="#代理具体做什么？" class="headerlink" title="代理具体做什么？"></a>代理具体做什么？</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">- 修改 HTTP 请求：url、header、body  </span><br><span class="line">- 过滤请求：根据一定的规则丢弃、过滤请求 </span><br><span class="line">- 决定转发到哪个后端（可以是静态定义的，也可以是动态决定） #nginx etc.</span><br><span class="line">- 保存服务器的应答，后续的请求可以直接使用保存的应答 # cache on proxy server</span><br><span class="line">- 修改应答：对应答做一些格式的转换，修改数据，甚至返回完全不一样的应答数据  #security issue?</span><br><span class="line">- 重试机制，如果后端服务器暂时无法响应，隔一段时间重试</span><br><span class="line">- ……</span><br></pre></td></tr></table></figure>



<p>普通代理</p>
<p>隧道代理</p>
<h2 id="代理和网关的区别"><a href="#代理和网关的区别" class="headerlink" title="代理和网关的区别"></a>代理和网关的区别</h2>]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>proxy</tag>
      </tags>
  </entry>
  <entry>
    <title>公司内部ssh设置socks代理</title>
    <url>/posts/a9be766e.html</url>
    <content><![CDATA[<blockquote>
<p><a href="https://intelpedia.intel.com/Proxy_at_Intel">Proxy at Intel - Intelpedia</a></p>
<p>因为公司设置了vpn，所以通过公司电脑访问外网时需要通过代理（http, https, socks等）。</p>
</blockquote>
<h4 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h4><p>在部署hexo时，用到了git deployer。这个deployer在每次<code>hexo deploy</code>的时候会通过ssh链接我的github账号的xxx.github.io项目，连接报错。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 看一下hexo的_config.yml文件</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat _config.yml  | grep -A10 deploy</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Docs: https://hexo.io/docs/one-command-deployment</span></span></span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: git@github.com:xxinran/xxinran.github.io.git</span><br><span class="line">  branch: main</span><br></pre></td></tr></table></figure>
<p>在sshkey上传成功的情况下，测试ssh连接github</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh -T git@github.com  #失败</span><br></pre></td></tr></table></figure>
<p>后来发现是ssh没有配置代理，有两个方案：</p>
<ol>
<li>设置http代理（失败了）</li>
<li>设置socks代理（成功）</li>
</ol>
<h4 id="如何设置socks代理"><a href="#如何设置socks代理" class="headerlink" title="如何设置socks代理"></a>如何设置socks代理</h4><p>在<a href="https://intelpedia.intel.com/Proxy_at_Intel#Using_openssh_to_connect_out_via_SOCKS_proxy">公司文档</a>中看到，可以通过设置socks代理，建立对外部的ssh连接。</p>
<p>需要修改<code>.ssh/config</code>文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> linux</span></span><br><span class="line">Host *</span><br><span class="line">   ProxyCommand nc -X 5 -x proxy-xxx.intel.com:1080 %h %p</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> windows上需要把nc命令改成connect，参数-S表示是socks代理。</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat ~/.ssh/config | tail -5</span></span><br><span class="line">Host github.com</span><br><span class="line">   HostName github.com</span><br><span class="line">   User git</span><br><span class="line">   ProxyCommand connect -S proxy-xxx.intel.com:1080 %h %p</span><br><span class="line">   ForwardAgent yes</span><br></pre></td></tr></table></figure>
<p>之后就可以成功ssh连接github了。</p>
<p>但是<code>ping proxy-xxx.intel.com</code>是ping不通的，应该是公司关闭了。</p>
]]></content>
      <categories>
        <category>deploy, network, intel</category>
      </categories>
      <tags>
        <tag>network</tag>
        <tag>ssh</tag>
        <tag>socks</tag>
      </tags>
  </entry>
  <entry>
    <title>科学上网ssr</title>
    <url>/posts/a838020d.html</url>
    <content><![CDATA[<blockquote>
<p>工具：<br>shadowsocks-libev<br>google vm</p>
</blockquote>
<h2 id="google-vm的基本配置"><a href="#google-vm的基本配置" class="headerlink" title="google vm的基本配置"></a>google vm的基本配置</h2><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>具体创建步骤略，需要一个绑定一个海外银行卡。</p>
<p>下面是我创建的虚拟机。</p>
<p><img src="/posts/a838020d/image-20210220162859580.png" alt="image-20210220162859580"></p>
<p>创建好之后可以去站长之家ping一下，保证在国内大多数地方可以ping通。如果ping不通删了重建。</p>
<p>然后通过左边的<code>ssh</code>登录上去。</p>
<h3 id="安装ssr"><a href="#安装ssr" class="headerlink" title="安装ssr"></a>安装ssr</h3><h4 id="dependency"><a href="#dependency" class="headerlink" title="dependency"></a>dependency</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt-get install --no-install-recommends gettext build-essential autoconf libtool libpcre3-dev asciidoc xmlto libev-dev libc-ares-dev automake libmbedtls-dev libsodium-dev</span><br></pre></td></tr></table></figure>
<h4 id="install-from-source"><a href="#install-from-source" class="headerlink" title="install from source"></a>install from source</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt install git</span><br><span class="line">git clone https://github.com/shadowsocks/shadowsocks-libev.git</span><br><span class="line">sudo apt-get install --no-install-recommends gettext build-essential autoconf libtool libpcre3-dev asciidoc xmlto libev-dev libc-ares-dev automake libmbedtls-dev libsodium-dev</span><br><span class="line">cd shadowsocks-libev/ ./autogen.sh</span><br></pre></td></tr></table></figure>
<p>missing makefile - &gt;  <code>git submodule update --init --recursive</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./configure</span><br><span class="line"><span class="meta">#</span><span class="bash"> maybe not need：</span></span><br><span class="line">make</span><br><span class="line">sudo make install</span><br><span class="line">dpkg-buildpackage -b -us -uc -I</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">报错: missing pkg-config</span></span><br><span class="line">sudo apt-get install -y pkg-config debhelper </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> install deb pkg</span></span><br><span class="line">cd .. &amp;&amp; sudo dpkg -i shadowsocks-libev*.deb</span><br></pre></td></tr></table></figure>
<h4 id="edit-config-file"><a href="#edit-config-file" class="headerlink" title="edit config file"></a>edit config file</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /etc/shadowsocks-libev/config.json </span><br><span class="line">&#123;</span><br><span class="line">  &quot;server&quot;:[&quot;::1&quot;, &quot;127.0.0.1&quot;], # internal IP</span><br><span class="line">  &quot;mode&quot;:&quot;tcp_and_udp&quot;,</span><br><span class="line">  &quot;server_port&quot;:8388, # need enable this port&#x27;s ingress in firewall</span><br><span class="line">  &quot;local_port&quot;:1080,</span><br><span class="line">  &quot;password&quot;:&quot;password&quot;,</span><br><span class="line">  &quot;timeout&quot;:86400,</span><br><span class="line">  &quot;method&quot;:&quot;aes-256-cfb&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> restart service</span></span><br><span class="line">sudo systemctl restart shadowsocks-libev</span><br></pre></td></tr></table></figure>
<h4 id="firewall-configuration"><a href="#firewall-configuration" class="headerlink" title="firewall configuration"></a>firewall configuration</h4><p><img src="/posts/a838020d/image-20210420112717314.png" alt="image-20210420112717314"></p>
<h3 id="安装客户端"><a href="#安装客户端" class="headerlink" title="安装客户端"></a>安装客户端</h3><h4 id="手机"><a href="#手机" class="headerlink" title="手机"></a>手机</h4><p>shadowrocket</p>
<h4 id="MacOS"><a href="#MacOS" class="headerlink" title="MacOS"></a>MacOS</h4><p>shadowsocks-NG</p>
<p>need local socks proxy</p>
]]></content>
      <tags>
        <tag>network</tag>
        <tag>deploy</tag>
      </tags>
  </entry>
  <entry>
    <title>tls和mtls比较</title>
    <url>/posts/425c0d17.html</url>
    <content><![CDATA[<h2 id="TLS"><a href="#TLS" class="headerlink" title="TLS"></a>TLS</h2><p>源于SSL，后改名TLS（Transport Layer Security）</p>
<ol>
<li>可以实现通讯双方的身份认证，应用数据加密。</li>
<li>与上层的应用层协议无耦合，应用层协议能透明地运行在TLS协议构建的安全通道之上。</li>
</ol>
<p>TLS由记录层（记录协议， record protocol）和握手协议层（握手协议、密钥变更协议、告警协议，应用数据协议）组成。</p>
<img src="/posts/425c0d17/image-20210329145853869.png" alt="image-20210329145853869" style="zoom:80%;">



<p><img src="https://segmentfault.com/img/bVbCCMD/view" alt="preview"></p>
<p>HTTPS = HTTP over TLS.</p>
<p>散列函数 Hash、对称加密和非对称加密，其利用非对称加密实现身份认证和密钥协商，对称加密算法采用协商的密钥对数据加密，基于散列函数验证信息的完整性</p>
<p>mTLS</p>
]]></content>
  </entry>
  <entry>
    <title>在mac上用vagrant和virtualbox创建虚拟机</title>
    <url>/posts/e1d7a8.html</url>
    <content><![CDATA[<h2 id="vagrant介绍"><a href="#vagrant介绍" class="headerlink" title="vagrant介绍"></a>vagrant介绍</h2><p>基于ruby，用于创建和部署虚拟化开发环境。它 使用Oracle的开源<a href="https://baike.baidu.com/item/VirtualBox">VirtualBox</a>虚拟化系统，使用 Chef创建自动化虚拟环境。我们可以使用它来干如下这些事：</p>
<ul>
<li><p>建立和删除虚拟机</p>
</li>
<li><p>配置虚拟机运行参数</p>
</li>
<li><p>管理虚拟机运行状态</p>
</li>
<li><p>自动配置和安装开发环境ß</p>
</li>
<li><p>打包和分发虚拟机运行环境</p>
</li>
</ul>
<p>　　Vagrant的运行，需要<strong>依赖</strong>某项具体的<strong>虚拟化技术</strong>，最常见的有VirtualBox以及VMWare两款，早期，Vagrant只支持VirtualBox，后来才加入了VMWare的支持。现在vagrant支持更多的虚拟化系统，包括libvirt，kvm，qemu，vmware，甚至docker。但还是以virtualbox为主。</p>
<img src="/posts/e1d7a8/image-20210128223212650.png" alt="image-20210128223212650" style="zoom:50%;">

<h3 id="vagrant和virtualbox"><a href="#vagrant和virtualbox" class="headerlink" title="vagrant和virtualbox"></a>vagrant和virtualbox</h3><p>virtualbox本身也可以创建vm，只是相对麻烦，vagrant可以调用virtualbox的接口更加方便的创建vm。</p>
<h3 id="在mac上安装vagrant-virtualbox"><a href="#在mac上安装vagrant-virtualbox" class="headerlink" title="在mac上安装vagrant+virtualbox"></a>在mac上安装vagrant+virtualbox</h3><h4 id="安装vagrant"><a href="#安装vagrant" class="headerlink" title="安装vagrant"></a>安装vagrant</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">brew install vagrant</span><br><span class="line">vagrant -v</span><br></pre></td></tr></table></figure>
<h4 id="安装virtualbox"><a href="#安装virtualbox" class="headerlink" title="安装virtualbox"></a>安装virtualbox</h4><p>国外的dmg文件下载很慢，我是国内随便找了一个<a href="https://ftp-new-pc.pconline.com.cn/pub/download/201909/pconline1567810102832.dmg?md5=dLIvaxUfMIGWrr-oloz2Jg&expires=1611979478">源</a>。</p>
<img src="/posts/e1d7a8/image-20210130123540411.png" alt="image-20210130123540411" style="zoom:50%;">

<p>virtualbox本身也可以创建vm，但是比较复杂。</p>
<h4 id="下载box"><a href="#下载box" class="headerlink" title="下载box"></a>下载box</h4><p>vagrant中box的概念，类似于docker image。我们可以通过指定box来启动一个vagrant vm。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 先下载box file到本地，再add</span></span><br><span class="line">vagrant box add centos8 /path/to/boxfile</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 直接从网上下载box file，并且add</span></span><br><span class="line">vagrant box add centos8 url_to_box_file</span><br></pre></td></tr></table></figure>
<p>vagrantbox的官方下载地址从国内连接很慢，我找到了一些国内的源。</p>
<blockquote>
<p>centos 8</p>
<p><a href="http://mirrors.ustc.edu.cn/centos-cloud/centos/8/vagrant/x86_64/images/CentOS-8-Vagrant-8.3.2011-20201204.2.x86_64.vagrant-virtualbox.box">http://mirrors.ustc.edu.cn/centos-cloud/centos/8/vagrant/x86_64/images/CentOS-8-Vagrant-8.3.2011-20201204.2.x86_64.vagrant-virtualbox.box</a></p>
<p>ubuntu1804</p>
<p><a href="https://mirrors.tuna.tsinghua.edu.cn/ubuntu-cloud-images/bionic/current/bionic-server-cloudimg-amd64-vagrant.box">https://mirrors.tuna.tsinghua.edu.cn/ubuntu-cloud-images/bionic/current/bionic-server-cloudimg-amd64-vagrant.box</a></p>
</blockquote>
<h4 id="启动虚拟机"><a href="#启动虚拟机" class="headerlink" title="启动虚拟机"></a>启动虚拟机</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vagrant box add centos8 http://mirrors.ustc.edu.cn/centos-cloud/centos/8/vagrant/x86_64/images/CentOS-8-Vagrant-8.3.2011-20201204.2.x86_64.vagrant-virtualbox.box</span></span><br><span class="line">==&gt; box: Box file was not detected as metadata. Adding it directly...</span><br><span class="line">==&gt; box: Adding box &#x27;centos8&#x27; (v0) for provider:</span><br><span class="line">    box: Downloading: http://mirrors.ustc.edu.cn/centos-cloud/centos/8/vagrant/x86_64/images/CentOS-8-Vagrant-8.3.2011-20201204.2.x86_64.vagrant-virtualbox.box</span><br><span class="line">==&gt; box: Successfully added box &#x27;centos8&#x27; (v0) for &#x27;virtualbox&#x27;!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> vagrant box list</span></span><br><span class="line">centos8    (virtualbox, 0)</span><br><span class="line">ubuntu1804 (virtualbox, 0)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建本地vagrant目录</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> vagrant</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这一步会创建一个vm的Vagrantfile，所有关于vm的配置都在这个文件里。</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vagrant init centos8</span></span><br><span class="line">A `Vagrantfile` has been placed in this directory. You are now</span><br><span class="line">ready to `vagrant up` your first virtual environment! Please read</span><br><span class="line">the comments in the Vagrantfile as well as documentation on</span><br><span class="line">`vagrantup.com` for more information on using Vagrant.</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash"> 启动vm</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vagrant up</span></span><br><span class="line">Bringing machine &#x27;default&#x27; up with &#x27;virtualbox&#x27; provider...</span><br><span class="line">==&gt; default: Importing base box &#x27;centos8&#x27;...</span><br><span class="line">==&gt; default: Matching MAC address for NAT networking...</span><br><span class="line">==&gt; default: Setting the name of the VM: vagrant_default_1611980890156_41411</span><br><span class="line">==&gt; default: Clearing any previously set network interfaces...</span><br><span class="line">==&gt; default: Preparing network interfaces based on configuration...</span><br><span class="line">    default: Adapter 1: nat</span><br><span class="line">    default: Adapter 2: hostonly</span><br><span class="line">==&gt; default: Forwarding ports...</span><br><span class="line">    default: 22 (guest) =&gt; 2222 (host) (adapter 1)  # 端口映射, 如果起了多个vagrant vm，那么各个</span><br><span class="line">    																								# vm 会映射到host的不同端口</span><br><span class="line">==&gt; default: Booting VM...</span><br><span class="line">==&gt; default: Waiting for machine to boot. This may take a few minutes...</span><br><span class="line">    default: SSH address: 127.0.0.1:2222</span><br><span class="line">    default: SSH username: vagrant</span><br><span class="line">    default: SSH auth method: private key</span><br><span class="line">    default:</span><br><span class="line">    default: Vagrant insecure key detected. Vagrant will automatically replace</span><br><span class="line">    default: this with a newly generated keypair for better security.</span><br><span class="line">    default:</span><br><span class="line">    default: Inserting generated public key within guest...</span><br><span class="line">    default: Removing insecure key from the guest if it&#x27;s present...</span><br><span class="line">    default: Key inserted! Disconnecting and reconnecting using new SSH key...</span><br><span class="line">==&gt; default: Machine booted and ready!</span><br><span class="line">==&gt; default: Checking for guest additions in VM...</span><br><span class="line">    default: No guest additions were detected on the base box for this VM! Guest</span><br><span class="line">    default: additions are required for forwarded ports, shared folders, host only</span><br><span class="line">    default: networking, and more. If SSH fails on this machine, please install</span><br><span class="line">    default: the guest additions and repackage the box to continue.</span><br><span class="line">    default:</span><br><span class="line">    default: This is not an error message; everything may continue to work properly,</span><br><span class="line">    default: in which case you may ignore this message.</span><br><span class="line">==&gt; default: Configuring and enabling network interfaces...</span><br><span class="line">==&gt; default: Rsyncing folder: /Users/xinran/vagrant/ =&gt; /vagrant</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 登入/退出vm</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vagrant ssh</span></span><br><span class="line">Last login: Sat Jan 30 04:28:57 2021 from 10.0.2.2</span><br><span class="line">[vagrant@localhost ~]$ </span><br><span class="line">[vagrant@localhost ~]$ exit</span><br><span class="line">logout</span><br><span class="line">Shared connection to 127.0.0.1 closed.</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看host上的2222端口</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> lsof -i:2222</span></span><br><span class="line">COMMAND     PID   USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME</span><br><span class="line">VBoxHeadl 13238 xinran   16u  IPv4 0x294cd01b85b191f5      0t0  TCP localhost:rockwell-csp2 (LISTEN)</span><br><span class="line">VBoxHeadl 13238 xinran   20u  IPv4 0x294cd01b95fd8815      0t0  TCP localhost:rockwell-csp2-&gt;localhost:63021 (ESTABLISHED)</span><br><span class="line">ssh       13880 xinran    3u  IPv4 0x294cd01b94ae9e35      0t0  TCP localhost:63021-&gt;localhost:rockwell-csp2 (ESTABLISHED)</span><br><span class="line">ssh       13894 xinran    3u  IPv4 0x294cd01b94ae9e35      0t0  TCP localhost:63021-&gt;localhost:rockwell-csp2 (ESTABLISHE</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: 在下载box image时，要找到对应provider的box。我第一次下载成了libvirt provider的box，然后vagrant init就失败了，因为我的host上装的是virtualbox作为provider。</p>
</blockquote>
]]></content>
      <tags>
        <tag>vagrant</tag>
        <tag>virtualbox</tag>
      </tags>
  </entry>
  <entry>
    <title>在centos8上使能vca-a卡</title>
    <url>/posts/dd7abf56.html</url>
    <content><![CDATA[<h2 id="vca卡的内部结构"><a href="#vca卡的内部结构" class="headerlink" title="vca卡的内部结构"></a>vca卡的内部结构</h2><p><img src="/posts/dd7abf56/image-20210202160154068.png" alt="image-20210202160154068"></p>
<ul>
<li><p>一块kabylake CPU，上面带一个Intel GFX 610集成显卡。</p>
</li>
<li><p>12个movidius VPU：</p>
<ul>
<li>其中两个通过USB连接到CPU上</li>
<li>剩下的10个通过PCI-USB bridge和CPU通信<ul>
<li>PCI-USB bridge ：The <strong>ASM1042A</strong> is ASMedia’s new generation of Universal Serial Bus 3.0 extended host controller, bridging PCI Express to two ports of USB3.0</li>
</ul>
</li>
</ul>
</li>
<li><p>一个PCIe switch：为host提供NTB</p>
<ul>
<li>NTB（Non-Transparent-Bridge）</li>
</ul>
</li>
</ul>
<h2 id="Host上vca-software安装"><a href="#Host上vca-software安装" class="headerlink" title="Host上vca software安装"></a>Host上vca software安装</h2><p><img src="/posts/dd7abf56/image-20210202162348217.png" alt="image-20210202162348217"></p>
<p>service跑在host上，agent和其他的workload都跑在vca卡上，卡上有个单独的os，ubuntu1804。</p>
<h3 id="下载software-packages源码"><a href="#下载software-packages源码" class="headerlink" title="下载software packages源码"></a>下载software packages源码</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/OpenVisualCloud/VCAC-SW-Analytics.git -b release/VCAC-A/R6</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tree -L 2</span></span><br><span class="line">.</span><br><span class="line">├── Documents                   # 平平无奇的文档</span><br><span class="line">│   ├── VCAC-Analytics-releasenotes-rev6-0.pdf</span><br><span class="line">│   └── VCAC-Analytics-software-installation-guide-rev6-0.pdf</span><br><span class="line">├── Intel_Media_Analytics_Host  # Host files</span><br><span class="line">│   ├── scripts                 # scripts to build kernel and vca driver module</span><br><span class="line">│   └── tar                     # kernel patch, vca driver patch and utilities</span><br><span class="line">└── Intel_Media_Analytics_Node  # Software packages</span><br><span class="line">    ├── scripts                 # scripte to build ubunut1804 image and dockerfiles </span><br><span class="line">    └── tar                     # patch for kernel on card, install pkg for VPU metrics</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="使用docker在容器里build安装包"><a href="#使用docker在容器里build安装包" class="headerlink" title="使用docker在容器里build安装包"></a>使用docker在容器里build安装包</h3><p>在host上 用docker启动容器，去build一些软件包，等等。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker info | grep Dir</span></span><br><span class="line">Docker Root Dir: /var/lib/docker</span><br></pre></td></tr></table></figure>
<p>这个docker目录最多只有50GB，不够去build vcad，可能会报错<code>&quot;no space left on device&quot; </code>, 所以我们需要换一个Docker Root Dir。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vim /usr/lib/systemd/system/docker.service<span class="string">&quot;</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> add <span class="string">&quot;--graph /home/docker&quot;</span> after <span class="string">&quot;ExecStart=/user/bin/dockerd</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /usr/lib/systemd/system/docker.service | grep graph</span></span><br><span class="line">ExecStart=/usr/bin/dockerd --graph /home/docker -H fd:// --containerd=/run/containerd/containerd.sock</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重启</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl daemon-reload</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl restart docker</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 再查看一下，已经改好了</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker info  | grep Dir</span></span><br><span class="line">Docker Root Dir: /home/docker</span><br></pre></td></tr></table></figure>


<p>拉取centos8的image</p>
<p><img src="/posts/dd7abf56/image-20210202170918413.png" alt="image-20210202170918413"></p>
<p>安装一些需要的pkg</p>
<p><img src="/posts/dd7abf56/image-20210202171007593.png" alt="image-20210202171007593"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[space@inspurserver CentOS8]$ docker ps</span><br><span class="line">CONTAINER ID   IMAGE                      COMMAND                  CREATED              STATUS              PORTS     NAMES</span><br><span class="line">d4ecdebab101   vcaa/centos-8.1-test:1.0   &quot;/home/space/VCAC-SW…&quot;   About a minute ago   Up About a minute             sleepy_ride</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">pwd</span>: /home/space/VCAC-SW-Analytics/VCAC-A/Intel_Media_Analytics_Host/scripts</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo ./build.sh</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">pwd</span>: /home/space/VCAC-SW-Analytics/VCAC-A/Intel_Media_Analytics_Node/scripts</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo ./vcad_build.sh -o EXTENDED  <span class="comment"># EXTENDED可以在ubuntu os里装上openvino等软件。</span></span></span><br></pre></td></tr></table></figure>
<h3 id="安装新kernel，vca软件等"><a href="#安装新kernel，vca软件等" class="headerlink" title="安装新kernel，vca软件等"></a>安装新kernel，vca软件等</h3><p>用生成的包，去换kernel，安装host上的vca daemon，vca cli， kernel， kernel-devel等等。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装kernel包</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo yum -y localinstall kernel-4.18.0-147_1.fb4dfe2.VCA+-1.x86_64.rpm</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo yum -y localinstall kernel-devel-4.18.0-147_1.fb4dfe2.VCA+-1.x86_64.rpm</span></span><br><span class="line"><span class="meta">#</span><span class="bash">安装vca driver module</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo yum -y localinstall vcass-modules-4.18.0-147_1.fb4dfe2.VCA</span></span><br><span class="line">+-1.690990a-0.x86_64.rpm</span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装vca daemon</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo yum -y localinstall daemon-vca-2.7.3-centos8-x86_64.rpm</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo yum -y localinstall ../../tar/Centos8/vca_query-1.0_centos8-1.x86_64.rpm</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 换default kernel，参考 https://www.golinuxcloud.com/change-default-kernel-version-rhel-centos-8/</span></span><br><span class="line">sudo grubby --set-default &quot;/boot/vmlinuz-4.18.0.0e222f9.VCA+&quot; </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> reboot</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo reboot</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开机之后验证kernel更换成功</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -r</span></span><br><span class="line">4.18.0.0e222f9.VCA+</span><br></pre></td></tr></table></figure>
<h3 id="enable-iommu"><a href="#enable-iommu" class="headerlink" title="enable iommu"></a>enable iommu</h3><p>其实我之前设置了<code>intel_iommu=on </code>，但是由于换了kernel，之前那个kernel的cmdline不起作用了。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在这里重新设置一下</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /boot/grub2/grubenv</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> GRUB Environment Block</span></span><br><span class="line">saved_entry=4f71b0d5929445139ae33f60b7e05767-4.18.0.0e222f9.VCA+</span><br><span class="line">kernelopts=root=LABEL=cloudimg-rootfs ro console=ttyS0,115200n8 no_timer_check net.ifnames=0 crashkernel=auto rd.auto=1 intel_iommu=on</span><br><span class="line">boot_success=1</span><br><span class="line">boot_indeterminate=0</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo reboot</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 然后就可以看到使能好的iommu groups了！</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls /sys/kernel/iommu_groups/</span></span><br><span class="line">0   11  14  17  2   22  25  28  30  33  36  39  41  44  47  5   52  55  58  60  63  66 </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">check cmdline</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /proc/cmdline</span></span><br><span class="line">BOOT_IMAGE=(hd0,gpt1)/boot/vmlinuz-4.18.0.0e222f9.VCA+ root=LABEL=cloudimg-rootfs ro console=ttyS0,115200n8 no_timer_check net.ifnames=0 crashkernel=auto rd.auto=1 intel_iommu=on</span><br></pre></td></tr></table></figure>
<h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 报错！因为不是root权限</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vcactl info BIOS 0 0</span></span><br><span class="line">ERROR: could not parse vca configuration file!</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo su</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 成功！</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vcactl status</span></span><br><span class="line">Card: 0 Cpu: 0  STATE: bios_up</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 报错，这个时候ubuntu1804和host的网络还没有配置好</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo vcactl network ip</span></span><br><span class="line">ERROR: Card: 0 Cpu: 0 - Card needs to be in &quot;os_ready&quot;, &quot;net_device_ready&quot;, &quot;dhcp_error&quot;, &quot;net_device_no_ip&quot;, &quot;dhcp_in_progress&quot; or &quot;dhcp_done&quot; state!</span><br></pre></td></tr></table></figure>
<h3 id="在vca卡上载入ubuntu-os"><a href="#在vca卡上载入ubuntu-os" class="headerlink" title="在vca卡上载入ubuntu os"></a>在vca卡上载入ubuntu os</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 关闭已打开的块设备</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vcactl blockio close 0 0 vcablk0</span></span><br><span class="line">WARNING: Card: 0 Cpu: 0 - Block device vcablk0 is not open. You do not need to close it.</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在build好的ubuntu的image加载进vca卡中</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vcactl blockio open 0 0 vcablk0 RW /home/space/VCAC-SW-Analytics/VCAC-A/Intel_Media_Analytics_Node/build/vcad/INSTALL/vca_disk48_k5.3_ubuntu18.04_1.0.1.vcad</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查看</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vcactl status</span></span><br><span class="line">Card: 0 Cpu: 0  STATE: bios_ready</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在bios_ready之后，需要重启一下vca卡，使其生效</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 关机</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vcactl pwrbtn-long 0 0</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vcactl status</span></span><br><span class="line">Card: 0 Cpu: 0  STATE: power_off</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 开机</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vcactl pwrbtn-short 0 0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> vcactl reset 0 0 --force</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vcactl boot 0 0 vcablk0 --force</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vcactl status</span></span><br><span class="line">Card: 0 Cpu: 0  STATE: booting</span><br><span class="line"><span class="meta">$</span><span class="bash"> vcactl status</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vcactl network ip</span></span><br><span class="line">Card 0 Cpu 0:</span><br><span class="line">172.32.1.1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 密码是vista1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ssh root@172.32.1.1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 到这里，vca卡的os也安装完毕了！</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>vca卡上os，可以和host相互ping通，</p>
<p>但连接不上internet。dns也没有设置好。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@vca_node_00:~# wget www.baidu.com</span><br><span class="line">--2021-02-02 15:06:13--  http://www.baidu.com/</span><br><span class="line">Resolving www.baidu.com (www.baidu.com)... failed: Temporary failure in name resolution.</span><br><span class="line">wget: unable to resolve host address &#x27;www.baidu.com&#x27;</span><br><span class="line">root@vca_node_00:~# ping baidu.com</span><br><span class="line">ping: baidu.com: Temporary failure in name resolution</span><br></pre></td></tr></table></figure>
<h3 id="vca-amp-host-网络配置"><a href="#vca-amp-host-网络配置" class="headerlink" title="vca &amp; host 网络配置"></a>vca &amp; host 网络配置</h3><p><strong>在host上</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> config proxy</span></span><br><span class="line">touch /etc/yum.repos.d/10proxy</span><br><span class="line">vim 10proxy</span><br><span class="line">Acquire::http::Proxy &quot; local network http proxy &quot;;</span><br><span class="line">scp /etc/yum.repos.d/10proxy root@172.32.1.1:/etc/apt/apt.conf.d/10proxy  </span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> config dns</span></span><br><span class="line">scp /etc/resolv.conf root@172.32.1.1:/etc</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">disable</span> firewall</span></span><br><span class="line">systemctl stop firewalld.service</span><br><span class="line">systemctl disable firewalld.service</span><br><span class="line">systemctl status firewalld.service</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">enable</span> ip forwarding <span class="keyword">in</span> host kernel</span></span><br><span class="line">echo 1 &gt; /proc/sys/net/ipv4/ip_forward</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> add rules to NAT</span></span><br><span class="line">iptables -t nat -A POSTROUTING -s 172.32.1.1 -d 0/0 -j MASQUERADE</span><br></pre></td></tr></table></figure>
<p><strong>在VCA卡上</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh root@172.32.1.1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> configure proxy</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> host没有proxy的时候，可以不设置</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vim ~/.bashrc</span></span><br><span class="line">export https_proxy=&quot; local network https proxy&quot;</span><br><span class="line">export http_proxy=&quot; local network http proxy&quot;</span><br><span class="line">export ftp_proxy=&quot; local network ftp proxy&quot;</span><br></pre></td></tr></table></figure>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget www.baidu.com</span><br><span class="line">wget www.google.com</span><br></pre></td></tr></table></figure>
<p>VCA卡应该和host有一样的网络连通能力。</p>
<h2 id="在vca卡上安装software"><a href="#在vca卡上安装software" class="headerlink" title="在vca卡上安装software"></a>在vca卡上安装software</h2><p><strong>Load SMBus driver</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> apt update</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> modprobe i2c-i801</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> modprobe i2c-dev</span>  </span><br></pre></td></tr></table></figure>
<p><strong>Load HDDL driver</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> insmod /lib/modules/5.3.18-1.9bf7f75.vca+/kernel/drivers/ion/myd_ion.ko</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> insmod /lib/modules/5.3.18-1.9bf7f75.vca+/kernel/drivers/usb/myd/myd_vsc.ko</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> lsmod | grep myd</span></span><br><span class="line">myd_vsc                24576  0</span><br><span class="line">myd_ion                49152  0</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>devices</category>
      </categories>
      <tags>
        <tag>device</tag>
        <tag>intel</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/posts/0.html</url>
    <content><![CDATA[<p>title: 在centos上修改源并安装docker<br>date: 2021-01-30 18:05:57<br>tags:</p>
<ul>
<li>deploy</li>
<li>docker<br>categories: deploy</li>
</ul>
<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>在mac上用vagrant起了一个centos8的vm。</p>
<h3 id="换系统yum源"><a href="#换系统yum源" class="headerlink" title="换系统yum源"></a>换系统yum源</h3><blockquote>
<p>这里换的是清华的源。</p>
<p> 参考：<a href="https://mirrors.tuna.tsinghua.edu.cn/help/centos/">https://mirrors.tuna.tsinghua.edu.cn/help/centos/</a></p>
</blockquote>
<p>建议先备份 <code>/etc/yum.repos.d/</code> 内的文件（CentOS 7 及之前为 <code>CentOS-Base.repo</code>，CentOS 8 为<code>CentOS-Linux-*.repo</code>）</p>
<p>然后编辑 <code>/etc/yum.repos.d/</code> 中的相应文件，在 <code>mirrorlist=</code> 开头行前面加 <code>#</code> 注释掉；并将 <code>baseurl=</code> 开头行取消注释（如果被注释的话），把该行内的域名（例如<code>mirror.centos.org</code>）替换为 <code>mirrors.tuna.tsinghua.edu.cn</code>。</p>
<p>以上步骤可以被下方的命令一步完成</p>
<figure class="highlight coq"><table><tr><td class="code"><pre><span class="line">sudo sed -e &#x27;s|<span class="type">^mirrorlist</span>=|<span class="type">#mirrorlist</span>=|<span class="type">g</span>&#x27; \</span><br><span class="line">         -e &#x27;s|<span class="type">^#baseurl</span>=http://mirror.centos.org|<span class="type">baseurl</span>=https://mirrors.tuna.tsinghua.edu.cn|<span class="type">g</span>&#x27; \</span><br><span class="line">         -i.bak \</span><br><span class="line">         /etc/yum.repos.d/CentOS-*.repo</span><br></pre></td></tr></table></figure>
<p>注意其中的<code>*</code>通配符，如果只需要替换一些文件中的源，请自行增删。</p>
<p>注意，如果需要启用其中一些 repo，需要将其中的 <code>enabled=0</code> 改为 <code>enabled=1</code>。</p>
<p>最后，更新软件包缓存</p>
<figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">sudo yum makecache</span></span><br></pre></td></tr></table></figure>
<p>这时下载常用的软件包就已经变快了。</p>
<h3 id="换下载docker的源"><a href="#换下载docker的源" class="headerlink" title="换下载docker的源"></a>换下载docker的源</h3><blockquote>
<p>参考：<a href="https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/">https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/</a></p>
</blockquote>
<p>然后我根据docker的官方文档安装docker，发现yum install docker-ce的时候特别慢。原因是官方文档配置的docker源repo是国外的，国外访问很慢。于是去网上找了清华的源。</p>
<p>如果你之前安装过 docker，请先删掉</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">sudo yum <span class="builtin-name">remove</span> docker docker-common docker-selinux docker-engine</span><br></pre></td></tr></table></figure>
<p>安装一些依赖</p>
<figure class="highlight gml"><table><tr><td class="code"><pre><span class="line">sudo yum install -<span class="symbol">y</span> yum-utils device-mapper-<span class="symbol">persistent</span>-data lvm2</span><br></pre></td></tr></table></figure>
<p>根据你的发行版下载repo文件: CentOS/RHEL Fedora</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">wget -O <span class="regexp">/etc/yum</span>.repos.d<span class="regexp">/docker-ce.repo https:/</span><span class="regexp">/download.docker.com/</span>linux<span class="regexp">/centos/</span>docker-ce.repo</span><br></pre></td></tr></table></figure>
<p>把软件仓库地址替换为 TUNA，这一步很关键，大部分网上的博客都没有提到，如果没有这一步，其实还是使用的国外的docker官方的源:</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">sudo sed -i <span class="string">&#x27;s+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+&#x27;</span> <span class="regexp">/etc/yum</span>.repos.d/docker-ce.repo</span><br></pre></td></tr></table></figure>
<p>最后安装:</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">sudo yum makecache</span><br><span class="line">sudo yum <span class="keyword">install</span> docker-ce</span><br></pre></td></tr></table></figure>
<h3 id="换docker镜像源"><a href="#换docker镜像源" class="headerlink" title="换docker镜像源"></a>换docker镜像源</h3><p>docker默认源在国外，同样很慢。我们可以通过修改docker配置更改docker image的源。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看docker daemon config</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls /etc/docker/daemon.json</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果文件不存在就创建一个</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo touch /etc/docker/daemon.json</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vi /etc/docker/daemon.json</span></span><br><span class="line">&#123;</span><br><span class="line">    &quot;registry-mirrors&quot; : [</span><br><span class="line">    &quot;https://registry.docker-cn.com&quot;,</span><br><span class="line">    &quot;https://docker.mirrors.ustc.edu.cn&quot;,</span><br><span class="line">    &quot;http://hub-mirror.c.163.com&quot;,</span><br><span class="line">    &quot;https://cr.console.aliyun.com/&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo systemctl restart docker.service</span></span><br></pre></td></tr></table></figure>


<h3 id="docker-permission-denied"><a href="#docker-permission-denied" class="headerlink" title="docker permission denied"></a>docker permission denied</h3><p>这个问题特别常见，就是我们在安装完docker之后，发现docker指令必须在root权限下运行，这时因为我们当前的用户没有被加入到docker组里。</p>
<h4 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h4><p>2.1、添加docker用户组</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">groupadd docker </span><br><span class="line"><span class="meta">#</span><span class="bash"> 一般都会提示docker用户组存在了。</span></span><br></pre></td></tr></table></figure>
<p>2.2、把当前用户加入docker用户组</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gpasswd -a $&#123;USER&#125; docker</span><br><span class="line"><span class="meta">#</span><span class="bash"> 或者 sudo usermod -aG docker <span class="variable">$&#123;USER&#125;</span> (未验证)</span></span><br></pre></td></tr></table></figure>
<p>3、查看是否添加成功：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat /etc/group | grep ^docker</span><br></pre></td></tr></table></figure>
<p>4、重启docker</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">serivce docker restart</span><br></pre></td></tr></table></figure>
<p>5、更新用户组</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">newgrp docker </span><br></pre></td></tr></table></figure>
<p>6、测试docker命令是否可以使用sudo正常使用</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker ps</span><br></pre></td></tr></table></figure>




<h1 id="docker-graph是什么东西啊"><a href="#docker-graph是什么东西啊" class="headerlink" title="docker graph是什么东西啊"></a>docker graph是什么东西啊</h1><p>The <code>-g</code> or <code>--graph</code> flag for the <code>dockerd</code> or <code>docker daemon</code> command was used to indicate the directory in which to store persistent data and resource configuration and has been replaced with the more descriptive <code>--data-root</code> flag.</p>
]]></content>
  </entry>
</search>
